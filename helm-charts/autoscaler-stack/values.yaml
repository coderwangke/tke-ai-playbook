lws:
  nameOverride: ""
  fullnameOverride: "tke-ai-lws"
  
  enablePrometheus: false
  enableCertManager: false
  
  replicaCount: 1
  imagePullSecrets: []
  
  # Customize controlerManager
  image:
    manager:
      repository: ccr.ccs.tencentyun.com/tke-market/k8s-lws-controller
      # tag, if defined will use the given image tag, else Chart.AppVersion will be used
      tag: v0.5.0
      pullPolicy: IfNotPresent
  
  podAnnotations: {}
  
  podSecurityContext:
    runAsNonRoot: true
    # fsGroup: 2000
  
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  
  service:
    type: ClusterIP
    port: 9443
  
  resources:
    requests:
      cpu: 1
      memory: 1Gi
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  
  nodeSelector: {}
  
  tolerations: []
  
  affinity: {}
  
dcgm-exporter:
  image:
    repository: nvcr.io/nvidia/k8s/dcgm-exporter
    pullPolicy: IfNotPresent
    # Image tag defaults to AppVersion, but you can use the tag key
    # for the image tag, e.g:
    tag: 4.2.3-4.1.1-ubuntu22.04
  
  # Change the following reference to "/etc/dcgm-exporter/default-counters.csv"
  # to stop profiling metrics from DCGM
  arguments: ["-f", "/etc/dcgm-exporter/default-counters.csv"]
  # NOTE: in general, add any command line arguments to arguments above
  # and they will be passed through.
  # Use "-r", "<HOST>:<PORT>" to connect to an already running hostengine
  # Example arguments: ["-r", "host123:5555"]
  # Use "-n" to remove the hostname tag from the output.
  # Example arguments: ["-n"]
  # Use "-d" to specify the devices to monitor. -d must be followed by a string
  # in the following format: [f] or [g[:numeric_range][+]][i[:numeric_range]]
  # Where a numeric range is something like 0-4 or 0,2,4, etc.
  # Example arguments: ["-d", "g+i"] to monitor all GPUs and GPU instances or
  # ["-d", "g:0-3"] to monitor GPUs 0-3.
  # Use "-m" to specify the namespace and name of a configmap containing
  # the watched exporter fields.
  # Example arguments: ["-m", "default:exporter-metrics-config-map"]
  
  # Image pull secrets for container images
  imagePullSecrets: []
  
  # Overrides the chart's name
  nameOverride: ""
  
  # Overrides the chart's computed fullname
  fullnameOverride: ""
  
  # Overrides the deployment namespace
  namespaceOverride: ""
  
  # Defines the runtime class that will be used by the pod
  runtimeClassName: ""
  # Defines serviceAccount names for components.
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name:
  
  rollingUpdate:
    # Specifies maximum number of DaemonSet pods that can be unavailable during the update
    maxUnavailable: 1
    # Specifies maximum number of nodes with an existing available DaemonSet pod that can have an updated DaemonSet pod during during an update
    maxSurge: 0
  
  # Labels to be added to dcgm-exporter pods
  podLabels: {}
  
  # Annotations to be added to dcgm-exporter pods
  podAnnotations: {}
  # Using this annotation which is required for prometheus scraping
    # prometheus.io/scrape: "true"
    # prometheus.io/port: "9400"
  
  # The SecurityContext for the dcgm-exporter pods
  podSecurityContext: {}
    # fsGroup: 2000
  
  # The SecurityContext for the dcgm-exporter containers
  securityContext:
    runAsNonRoot: false
    runAsUser: 0
    capabilities:
      add: ["SYS_ADMIN"]
    # readOnlyRootFilesystem: true
  
  # Defines the dcgm-exporter service
  service:
    # When enabled, the helm chart will create service
    enable: true
    type: ClusterIP
    # Accepts either "Cluster" or "Local", choose Local if you want to route internal traffic within the node only
    internalTrafficPolicy: Cluster
    clusterIP: ""
    port: 9400
    address: ":9400"
    # Annotations to add to the service
    annotations: {}
  
  # Allows to control pod resources
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  serviceMonitor:
    apiVersion: "monitoring.coreos.com/v1"
    enabled: true
    interval: 15s
    honorLabels: false
    additionalLabels: {}
      #monitoring: prometheus
    relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
  
  nodeSelector: 
    nvidia-device-enable: enable
    #node: gpu
  
  tolerations: []
  #- operator: Exists
  
  affinity: {}
    #nodeAffinity:
    #  requiredDuringSchedulingIgnoredDuringExecution:
    #    nodeSelectorTerms:
    #    - matchExpressions:
    #      - key: nvidia-gpu
    #        operator: Exists
  
  extraHostVolumes: []
  #- name: host-binaries
  #  hostPath: /opt/bin
  
  extraConfigMapVolumes:
    - name: exporter-metrics-volume
      configMap:
        name: exporter-metrics-config-map
        items:
        - key: metrics
          path: default-counters.csv
  
  extraVolumeMounts:
    - name: exporter-metrics-volume
      mountPath: /etc/dcgm-exporter/default-counters.csv
      subPath: default-counters.csv
  
  extraEnv: []
  #- name: EXTRA_VAR
  #  value: "TheStringValue"
  
  # Path to the kubelet socket for /pod-resources
  kubeletPath: "/var/lib/kubelet/pod-resources"
  
  # HTTPS configuration
  tlsServerConfig:
    # Enable or disable HTTPS configuration
    enabled: false
    # Use autogenerated self-signed TLS certificates. Not recommended for production environments.
    autoGenerated: true
    # Existing secret containing your own server key and certificate
    existingSecret: ""
    # Certificate file name
    certFilename: "tls.crt"
    # Key file name
    keyFilename: "tls.key"
    # CA certificate file name
    caFilename: "ca.crt"
    # Server policy for client authentication. Maps to ClientAuth Policies.
    # For more detail on clientAuth options:
    # https://golang.org/pkg/crypto/tls/#ClientAuthType
    #
    # NOTE: If you want to enable client authentication, you need to use
    # RequireAndVerifyClientCert. Other values are insecure.
    clientAuthType: ""
    # TLS Key for HTTPS - ignored if existingSecret is provided
    key: ""
    # TLS Certificate for HTTPS - ignored if existingSecret is provided
    cert: ""
    # CA Certificate for HTTPS - ignored if existingSecret is provided
    ca: ""
  
  basicAuth:
    #Object containing <user>:<passwords> key-value pairs for each user that will have access via basic authentication
    users: {}
  
  # Customized list of metrics to emit. Expected to be in the same format (CSV) as the default list.
  # Must be the complete list and is not additive. If unset, the default list will take effect.
  # customMetrics: |
    # Format
    # If line starts with a '#' it is considered a comment
    # DCGM FIELD, Prometheus metric type, help message
    
    # Clocks
    # DCGM_FI_DEV_SM_CLOCK,  gauge, SM clock frequency (in MHz).
    # DCGM_FI_DEV_MEM_CLOCK, gauge, Memory clock frequency (in MHz).
    
    # Temperature
    # DCGM_FI_DEV_MEMORY_TEMP, gauge, Memory temperature (in C).
    # DCGM_FI_DEV_GPU_TEMP,    gauge, GPU temperature (in C).
    
    # Power
    # DCGM_FI_DEV_POWER_USAGE,              gauge, Power draw (in W).
    # DCGM_FI_DEV_TOTAL_ENERGY_CONSUMPTION, counter, Total energy consumption since boot (in mJ).
    
    # PCIE
    # DCGM_FI_DEV_PCIE_TX_THROUGHPUT,  counter, Total number of bytes transmitted through PCIe TX (in KB) via NVML.
    # DCGM_FI_DEV_PCIE_RX_THROUGHPUT,  counter, Total number of bytes received through PCIe RX (in KB) via NVML.
    # DCGM_FI_DEV_PCIE_REPLAY_COUNTER, counter, Total number of PCIe retries.
    
    # Utilization (the sample period varies depending on the product)
    # DCGM_FI_DEV_GPU_UTIL,      gauge, GPU utilization (in %).
    # DCGM_FI_DEV_MEM_COPY_UTIL, gauge, Memory utilization (in %).
    # DCGM_FI_DEV_ENC_UTIL,      gauge, Encoder utilization (in %).
    # DCGM_FI_DEV_DEC_UTIL ,     gauge, Decoder utilization (in %).
    
    # Errors and violations
    # DCGM_FI_DEV_XID_ERRORS,            gauge,   Value of the last XID error encountered.
    # DCGM_FI_DEV_POWER_VIOLATION,       counter, Throttling duration due to power constraints (in us).
    # DCGM_FI_DEV_THERMAL_VIOLATION,     counter, Throttling duration due to thermal constraints (in us).
    # DCGM_FI_DEV_SYNC_BOOST_VIOLATION,  counter, Throttling duration due to sync-boost constraints (in us).
    # DCGM_FI_DEV_BOARD_LIMIT_VIOLATION, counter, Throttling duration due to board limit constraints (in us).
    # DCGM_FI_DEV_LOW_UTIL_VIOLATION,    counter, Throttling duration due to low utilization (in us).
    # DCGM_FI_DEV_RELIABILITY_VIOLATION, counter, Throttling duration due to reliability constraints (in us).
    
    # Memory usage
    # DCGM_FI_DEV_FB_FREE, gauge, Framebuffer memory free (in MiB).
    # DCGM_FI_DEV_FB_USED, gauge, Framebuffer memory used (in MiB).
    
    # ECC
    # DCGM_FI_DEV_ECC_SBE_VOL_TOTAL, counter, Total number of single-bit volatile ECC errors.
    # DCGM_FI_DEV_ECC_DBE_VOL_TOTAL, counter, Total number of double-bit volatile ECC errors.
    # DCGM_FI_DEV_ECC_SBE_AGG_TOTAL, counter, Total number of single-bit persistent ECC errors.
    # DCGM_FI_DEV_ECC_DBE_AGG_TOTAL, counter, Total number of double-bit persistent ECC errors.
    
    # Retired pages
    # DCGM_FI_DEV_RETIRED_SBE,     counter, Total number of retired pages due to single-bit errors.
    # DCGM_FI_DEV_RETIRED_DBE,     counter, Total number of retired pages due to double-bit errors.
    # DCGM_FI_DEV_RETIRED_PENDING, counter, Total number of pages pending retirement.
    
    # NVLink
    # DCGM_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_TOTAL, counter, Total number of NVLink flow-control CRC errors.
    # DCGM_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_TOTAL, counter, Total number of NVLink data CRC errors.
    # DCGM_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_TOTAL,   counter, Total number of NVLink retries.
    # DCGM_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_TOTAL, counter, Total number of NVLink recovery errors.
    # DCGM_FI_DEV_NVLINK_BANDWIDTH_TOTAL,            counter, Total number of NVLink bandwidth counters for all lanes.
    # DCGM_FI_DEV_NVLINK_BANDWIDTH_L0,               counter, The number of bytes of active NVLink rx or tx data including both header and payload.
    
    # VGPU License status
    # DCGM_FI_DEV_VGPU_LICENSE_STATUS, gauge, vGPU License status
    
    # Remapped rows
    # DCGM_FI_DEV_UNCORRECTABLE_REMAPPED_ROWS, counter, Number of remapped rows for uncorrectable errors
    # DCGM_FI_DEV_CORRECTABLE_REMAPPED_ROWS,   counter, Number of remapped rows for correctable errors
    # DCGM_FI_DEV_ROW_REMAP_FAILURE,           gauge,   Whether remapping of rows has failed
    
    # DCP metrics
    # DCGM_FI_PROF_GR_ENGINE_ACTIVE,   gauge, Ratio of time the graphics engine is active.
    # DCGM_FI_PROF_SM_ACTIVE,          gauge, The ratio of cycles an SM has at least 1 warp assigned.
    # DCGM_FI_PROF_SM_OCCUPANCY,       gauge, The ratio of number of warps resident on an SM.
    # DCGM_FI_PROF_PIPE_TENSOR_ACTIVE, gauge, Ratio of cycles the tensor (HMMA) pipe is active.
    # DCGM_FI_PROF_DRAM_ACTIVE,        gauge, Ratio of cycles the device memory interface is active sending or receiving data.
    # DCGM_FI_PROF_PIPE_FP64_ACTIVE,   gauge, Ratio of cycles the fp64 pipes are active.
    # DCGM_FI_PROF_PIPE_FP32_ACTIVE,   gauge, Ratio of cycles the fp32 pipes are active.
    # DCGM_FI_PROF_PIPE_FP16_ACTIVE,   gauge, Ratio of cycles the fp16 pipes are active.
    # DCGM_FI_PROF_PCIE_TX_BYTES,      counter, The number of bytes of active pcie tx data including both header and payload.
    # DCGM_FI_PROF_PCIE_RX_BYTES,      counter, The number of bytes of active pcie rx data including both header and payload.
  

prometheus-adapter:
  nameOverride: ""
  affinity: {}
  topologySpreadConstraints: []
  image:
    repository: ccr.ccs.tencentyun.com/halewang/prometheus-adapter
    # if not set appVersion field from Chart.yaml is used
    tag: "v0.12.0"
    pullPolicy: IfNotPresent
    pullSecrets: []
      # - foo
  logLevel: 4
  metricsRelistInterval: 1m
  
  listenPort: 6443
  nodeSelector: {}
  priorityClassName: ""
  ## Override the release namespace (for multi-namespace deployments in combined charts)
  namespaceOverride: ""
  ## Additional annotations to add to all resources
  customAnnotations: {}
    # role: custom-metrics
  ## Additional labels to add to all resources
  customLabels: {}
    # monitoring: prometheus-adapter
  
  # Url to access prometheus
  prometheus:
    # Value is templated
    url: http://tke-ai-prometheus-prometheus
    port: 9090
    path: ""
  
  replicas: 1
  
  # k8s 1.21 needs fsGroup to be set for non root deployments
  # ref: https://github.com/kubernetes/kubernetes/issues/70679
  podSecurityContext:
    fsGroup: 10001
  
  # SecurityContext of the container
  # ref. https://kubernetes.io/docs/tasks/configure-pod-container/security-context
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 10001
    seccompProfile:
      type: RuntimeDefault
  
  rbac:
    # Specifies whether RBAC resources should be created
    create: true
    # Specifies if a Cluster Role should be used for the Auth Reader
    useAuthReaderClusterRole: false
    externalMetrics:
      resources: ["*"]
    customMetrics:
      resources: ["*"]
  # If false then the user will opt out of automounting API credentials.
  automountServiceAccountToken: true
  
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name:
    # ServiceAccount annotations.
    # Use case: AWS EKS IAM roles for service accounts
    # ref: https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html
    annotations: {}
    # If false then the user will opt out of automounting API credentials.
    automountServiceAccountToken: true
  
  # Custom DNS configuration to be added to prometheus-adapter pods
  dnsConfig: {}
    # nameservers:
    #   - 1.2.3.4
    # searches:
    #   - ns1.svc.cluster-domain.example
    #   - my.dns.search.suffix
    # options:
    #   - name: ndots
    #     value: "2"
    #   - name: edns0
  
  resources: {}
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
  
  # Configure liveness probe
  # https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#Probe
  livenessProbe:
    httpGet:
      path: /healthz
      port: https
      scheme: HTTPS
    initialDelaySeconds: 30
    timeoutSeconds: 5
  
  # Configure readiness probe
  readinessProbe:
    httpGet:
      path: /healthz
      port: https
      scheme: HTTPS
    initialDelaySeconds: 30
    timeoutSeconds: 5
  
  # Configure startup probe
  # Use if prometheus-adapter takes a long time to finish startup e.g. polling a lot of API versions in cluster
  startupProbe: {}
  
  rules:
    default: true
  
    custom: []
      # - seriesQuery: '{__name__=~"^some_metric_count$"}'
      #   resources:
      #     template: <<.Resource>>
      #   name:
      #     matches: ""
      #     as: "my_custom_metric"
      #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
  
    # Mounts a configMap with pre-generated rules for use. Overrides the
    # default, custom, external and resource entries
    existing:
  
    external: []
      # - seriesQuery: '{__name__=~"^some_metric_count$"}'
      #   resources:
      #     template: <<.Resource>>
      #   name:
      #     matches: ""
      #     as: "my_external_metric"
      #   metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)
  
    # resource:
    #   cpu:
    #     containerQuery: |
    #       sum by (<<.GroupBy>>) (
    #         rate(container_cpu_usage_seconds_total{container!="",<<.LabelMatchers>>}[3m])
    #       )
    #     nodeQuery: |
    #       sum  by (<<.GroupBy>>) (
    #         rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",<<.LabelMatchers>>}[3m])
    #       )
    #     resources:
    #       overrides:
    #         node:
    #           resource: node
    #         namespace:
    #           resource: namespace
    #         pod:
    #           resource: pod
    #     containerLabel: container
    #   memory:
    #     containerQuery: |
    #       sum by (<<.GroupBy>>) (
    #         avg_over_time(container_memory_working_set_bytes{container!="",<<.LabelMatchers>>}[3m])
    #       )
    #     nodeQuery: |
    #       sum by (<<.GroupBy>>) (
    #         avg_over_time(node_memory_MemTotal_bytes{<<.LabelMatchers>>}[3m])
    #         -
    #         avg_over_time(node_memory_MemAvailable_bytes{<<.LabelMatchers>>}[3m])
    #       )
    #     resources:
    #       overrides:
    #         node:
    #           resource: node
    #         namespace:
    #           resource: namespace
    #         pod:
    #           resource: pod
    #     containerLabel: container
    #   window: 3m
  
  service:
    annotations: {}
    port: 443
    type: ClusterIP
    # clusterIP: 1.2.3.4
    ipDualStack:
      enabled: false
      ipFamilies: ["IPv6", "IPv4"]
      ipFamilyPolicy: "PreferDualStack"
  tls:
    enable: false
    ca: |-
      # Public CA file that signed the APIService
    key: |-
      # Private key of the APIService
    certificate: |-
      # Public key of the APIService
  
  # Set environment variables from secrets, configmaps or by setting them as name/value
  env: []
    # - name: TMP_DIR
    #   value: /tmp
    # - name: PASSWORD
    #   valueFrom:
    #     secretKeyRef:
    #       name: mysecret
    #       key: password
    #       optional: false
  
  # Any extra arguments
  extraArguments: []
    # - --tls-private-key-file=/etc/tls/tls.key
    # - --tls-cert-file=/etc/tls/tls.crt
  
  # Additional containers to add to the pod
  extraContainers: []
  
  # Any extra volumes
  extraVolumes: []
    # - name: example-name
    #   hostPath:
    #     path: /path/on/host
    #     type: DirectoryOrCreate
    # - name: ssl-certs
    #   hostPath:
    #     path: /etc/ssl/certs/ca-bundle.crt
    #     type: File
  
  # Any extra volume mounts
  extraVolumeMounts: []
    #   - name: example-name
    #     mountPath: /path/in/container
    #   - name: ssl-certs
    #     mountPath: /etc/ssl/certs/ca-certificates.crt
    #     readOnly: true
  
  tolerations: []
  
  # Labels added to the pod
  podLabels: {}
  
  # Annotations added to the pod
  podAnnotations: {}
  
  # Annotations added to the deployment
  deploymentAnnotations: {}
  
  hostNetwork:
    # Specifies if prometheus-adapter should be started in hostNetwork mode.
    #
    # You would require this enabled if you use alternate overlay networking for pods and
    # API server unable to communicate with metrics-server. As an example, this is required
    # if you use Weave network on EKS. See also dnsPolicy
    enabled: false
  
  # When hostNetwork is enabled, you probably want to set this to ClusterFirstWithHostNet
  # dnsPolicy: ClusterFirstWithHostNet
  
  # Deployment strategy type
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  
  podDisruptionBudget:
    # Specifies if PodDisruptionBudget should be enabled
    # When enabled, minAvailable or maxUnavailable should also be defined.
    enabled: false
    minAvailable:
    maxUnavailable: 1
  
  certManager:
    enabled: false
    caCertDuration: 43800h0m0s
    certDuration: 8760h0m0s
    # -- Set the revisionHistoryLimit on the Certificates. See
    # https://cert-manager.io/docs/reference/api-docs/#cert-manager.io/v1.CertificateSpec
    # Defaults to nil.
    caCertRevisionHistoryLimit:
    certRevisionHistoryLimit:
  
  ## Extra manifests to deploy as an array
  extraManifests: []
    # - apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #   labels:
    #     name: prometheus-extra
    #   data:
    #     extra-data: "value"

argo-workflow:
  images:
    # -- Common tag for Argo Workflows images. Defaults to `.Chart.AppVersion`.
    tag: ""
    # -- imagePullPolicy to apply to all containers
    pullPolicy: Always
    # -- Secrets with credentials to pull images from a private registry
    pullSecrets: []
    # - name: argo-pull-secret
  
  ## Custom resource configuration
  crds:
    # -- Install and upgrade CRDs
    install: true
    # -- Keep CRDs on chart uninstall
    keep: false
    # -- Annotations to be added to all CRDs
    annotations: {}
  
  # -- Create clusterroles that extend existing clusterroles to interact with argo-cd crds
  ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
  createAggregateRoles: true
  
  # -- String to partially override "argo-workflows.fullname" template
  nameOverride: ""
  
  # -- String to fully override "argo-workflows.fullname" template
  fullnameOverride: tke-ai
  
  # -- Override the namespace
  # @default -- `.Release.Namespace`
  namespaceOverride: ""
  
  # -- Labels to set on all resources
  commonLabels: {}
  
  # -- Override the Kubernetes version, which is used to evaluate certain manifests
  kubeVersionOverride: ""
  
  # Override APIVersions
  apiVersionOverrides:
    # -- String to override apiVersion of autoscaling rendered by this helm chart
    autoscaling: "" # autoscaling/v2
    # -- String to override apiVersion of GKE resources rendered by this helm chart
    cloudgoogle: "" # cloud.google.com/v1
    # -- String to override apiVersion of monitoring CRDs (ServiceMonitor) rendered by this helm chart
    monitoring: "" # monitoring.coreos.com/v1
  
  # -- Restrict Argo to operate only in a single namespace (the namespace of the
  # Helm release) by apply Roles and RoleBindings instead of the Cluster
  # equivalents, and start workflow-controller with the --namespaced flag. Use it
  # in clusters with strict access policy.
  singleNamespace: false
  
  workflow:
    # -- Deprecated; use controller.workflowNamespaces instead.
    namespace:
    serviceAccount:
      # -- Specifies whether a service account should be created
      create: false
      # -- Labels applied to created service account
      labels: {}
      # -- Annotations applied to created service account
      annotations: {}
      # -- Service account which is used to run workflows
      name: "argo-workflow"
      # -- Secrets with credentials to pull images from a private registry. Same format as `.Values.images.pullSecrets`
      pullSecrets: []
    rbac:
      # -- Adds Role and RoleBinding for the above specified service account to be able to run workflows.
      # A Role and Rolebinding pair is also created for each namespace in controller.workflowNamespaces (see below)
      create: true
      # -- Allows permissions for the Argo Agent. Only required if using http/plugin templates
      agentPermissions: false
      # -- Allows permissions for the Argo Artifact GC pod. Only required if using artifact gc
      artifactGC: false
      # -- Extra service accounts to be added to the RoleBinding
      serviceAccounts: []
        # - name: my-service-account
        #   namespace: my-namespace
  
  controller:
    image:
      # -- Registry to use for the controller
      registry: ccr.ccs.tencentyun.com
      # -- Registry to use for the controller
      repository: tke-market/argo-workflow-controller
      # -- Image tag for the workflow controller. Defaults to `.Values.images.tag`.
      tag: ""
    # -- parallelism dictates how many workflows can be running at the same time
    parallelism:
    # -- Globally limits the rate at which pods are created.
    # This is intended to mitigate flooding of the Kubernetes API server by workflows with a large amount of
    # parallel nodes.
    resourceRateLimit: {}
      # limit: 10
      # burst: 1
  
    rbac:
      # -- Adds Role and RoleBinding for the controller.
      create: true
      # -- Allows controller to get, list, and watch certain k8s secrets
      secretWhitelist: []
      # -- Allows controller to get, list and watch all k8s secrets. Can only be used if secretWhitelist is empty.
      accessAllSecrets: false
      # -- Allows controller to create and update ConfigMaps. Enables memoization feature
      writeConfigMaps: false
  
    configMap:
      # -- Create a ConfigMap for the controller
      create: true
      # -- ConfigMap name
      name: ""
      # -- ConfigMap annotations
      annotations: {}
  
    # -- Limits the maximum number of incomplete workflows in a namespace
    namespaceParallelism:
    # -- Resolves ongoing, uncommon AWS EKS bug: https://github.com/argoproj/argo-workflows/pull/4224
    initialDelay:
    # -- deploymentAnnotations is an optional map of annotations to be applied to the controller Deployment
    deploymentAnnotations: {}
    # -- podAnnotations is an optional map of annotations to be applied to the controller Pods
    podAnnotations: {}
    # -- Optional labels to add to the controller pods
    podLabels: {}
    # -- SecurityContext to set on the controller pods
    podSecurityContext: {}
    # podPortName: http
    metricsConfig:
      # -- Enables prometheus metrics server
      enabled: false

    # -- Number of workflow workers
    workflowWorkers: # 32
    # -- Number of workflow TTL workers
    workflowTTLWorkers: # 4
    # -- Number of pod cleanup workers
    podCleanupWorkers: # 4
    # -- Number of cron workflow workers
    # Only valid for 3.5+
    cronWorkflowWorkers: # 8
    # -- Restricts the Workflows that the controller will process.
    # Only valid for 2.9+
    workflowRestrictions: {}
      # templateReferencing: Strict|Secure
  
    # telemetryConfig controls the path and port for prometheus telemetry. Telemetry is enabled and emitted in the same endpoint
    # as metrics by default, but can be overridden using this config.
    telemetryConfig:
      # -- Enables prometheus telemetry server
      enabled: false
      # -- telemetry path
    serviceMonitor:
      # -- Enable a prometheus ServiceMonitor
      enabled: false
      # -- Prometheus ServiceMonitor labels
      additionalLabels: {}
      # -- Prometheus ServiceMonitor namespace
      namespace: "" # "monitoring"
    serviceAccount:
      # -- Create a service account for the controller
      create: true
      # -- Service account name
      name: ""
      # -- Labels applied to created service account
      labels: {}
      # -- Annotations applied to created service account
      annotations: {}
  
    # -- Workflow controller name string
    name: workflow-controller
  
    # -- Specify all namespaces where this workflow controller instance will manage
    # workflows. This controls where the service account and RBAC resources will
    # be created. Only valid when singleNamespace is false.
    workflowNamespaces:
      - default
  
    instanceID:
      # -- Configures the controller to filter workflow submissions
      # to only those which have a matching instanceID attribute.
      ## NOTE: If `instanceID.enabled` is set to `true` then either `instanceID.userReleaseName`
      ## or `instanceID.explicitID` must be defined.
      enabled: false
      # -- Use ReleaseName as instanceID
      useReleaseName: false
      # useReleaseName: true
  
      # -- Use a custom instanceID
      explicitID: ""
      # explicitID: unique-argo-controller-identifier
  
    logging:
      # -- Set the logging level (one of: `debug`, `info`, `warn`, `error`)
      level: info
      # -- Set the glog logging level
      globallevel: "0"
      # -- Set the logging format (one of: `text`, `json`)
      format: "text"
  
    # -- Service type of the controller Service
    serviceType: ClusterIP
    # -- Annotations to be applied to the controller Service
    serviceAnnotations: {}
    # -- Optional labels to add to the controller Service
    serviceLabels: {}
    # -- The class of the load balancer implementation
    loadBalancerClass: ""
    # -- Source ranges to allow access to service from. Only applies to service type `LoadBalancer`
    loadBalancerSourceRanges: []
  
    # -- Resource limits and requests for the controller
    resources: {}
  
    # -- Configure liveness [probe] for the controller
    # @default -- See [values.yaml]
    livenessProbe:
      httpGet:
        port: 6060
        path: /healthz
      failureThreshold: 3
      initialDelaySeconds: 90
      periodSeconds: 60
      timeoutSeconds: 30
  
    # -- Extra environment variables to provide to the controller container
    extraEnv: []
      # - name: FOO
      #   value: "bar"
  
    # -- Extra arguments to be added to the controller
    extraArgs: []
    # -- Additional volume mounts to the controller main container
    volumeMounts: []
    # -- Additional volumes to the controller pod
    volumes: []
    # -- The number of controller pods to run
    replicas: 1
    # -- The number of revisions to keep.
    revisionHistoryLimit: 10
  
    pdb:
      # -- Configure [Pod Disruption Budget] for the controller pods
      enabled: false
      # minAvailable: 1
      # maxUnavailable: 1
  
    # -- [Node selector]
    nodeSelector:
      kubernetes.io/os: linux
    # -- [Tolerations] for use with node taints
    tolerations: []
    # -- Assign custom [affinity] rules
    affinity: {}
  
    # -- Assign custom [TopologySpreadConstraints] rules to the workflow controller
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment
    topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
  
    # -- Leverage a PriorityClass to ensure your pods survive resource shortages.
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    priorityClassName: ""
  
    # -- Configure Argo Server to show custom [links]
    ## Ref: https://argo-workflows.readthedocs.io/en/stable/links/
    links: []
    # -- Configure Argo Server to show custom [columns]
    ## Ref: https://github.com/argoproj/argo-workflows/pull/10693
    columns: []
    # -- Set ui navigation bar background color
    navColor: ""
    clusterWorkflowTemplates:
      # -- Create a ClusterRole and CRB for the controller to access ClusterWorkflowTemplates.
      enabled: true
      # -- Extra service accounts to be added to the ClusterRoleBinding
      serviceAccounts: []
        # - name: my-service-account
        #   namespace: my-namespace
    # -- Extra containers to be added to the controller deployment
    extraContainers: []
  
    # -- Enables init containers to be added to the controller deployment
    extraInitContainers: []
  
    # -- Workflow retention by number of workflows
    retentionPolicy: {}
    #  completed: 10
    #  failed: 3
    #  errored: 3
  
    nodeEvents:
      # -- Enable to emit events on node completion.
      ## This can take up a lot of space in k8s (typically etcd) resulting in errors when trying to create new events:
      ## "Unable to create audit event: etcdserver: mvcc: database space exceeded"
      enabled: true
  
    workflowEvents:
      # -- Enable to emit events on workflow status changes.
      ## This can take up a lot of space in k8s (typically etcd), resulting in errors when trying to create new events:
      ## "Unable to create audit event: etcdserver: mvcc: database space exceeded"
      enabled: true
  
    # -- Configure when workflow controller runs in a different k8s cluster with the workflow workloads,
    # or needs to communicate with the k8s apiserver using an out-of-cluster kubeconfig secret.
    # @default -- `{}` (See [values.yaml])
    kubeConfig: {}
      # # name of the kubeconfig secret, may not be empty when kubeConfig specified
      # secretName: kubeconfig-secret
      # # key of the kubeconfig secret, may not be empty when kubeConfig specified
      # secretKey: kubeconfig
      # # mounting path of the kubeconfig secret, default to /kube/config
      # mountPath: /kubeconfig/mount/path
      # # volume name when mounting the secret, default to kubeconfig
      # volumeName: kube-config-volume
  
    # -- Specifies the duration in seconds before a terminating pod is forcefully killed. A zero value indicates that the pod will be forcefully terminated immediately.
    # @default -- `30` seconds (Kubernetes default)
    podGCGracePeriodSeconds:
  
    # -- The duration in seconds before the pods in the GC queue get deleted. A zero value indicates that the pods will be deleted immediately.
    # @default -- `5s` (Argo Workflows default)
    podGCDeleteDelayDuration: ""
  
  # mainContainer adds default config for main container that could be overriden in workflows template
  mainContainer:
    # -- imagePullPolicy to apply to Workflow main container. Defaults to `.Values.images.pullPolicy`.
    imagePullPolicy: ""
    # -- Resource limits and requests for the Workflow main container
    resources: {}
    # -- Adds environment variables for the Workflow main container
    env: []
    # -- Adds reference environment variables for the Workflow main container
    envFrom: []
    # -- sets security context for the Workflow main container
    securityContext: {}
  
  # executor controls how the init and wait container should be customized
  executor:
    image:
      # -- Registry to use for the Workflow Executors
      registry: ccr.ccs.tencentyun.com
      # -- Repository to use for the Workflow Executors
      repository: tke-market/argoexec
      # -- Image tag for the workflow executor. Defaults to `.Values.images.tag`.
      tag: ""
      # -- Image PullPolicy to use for the Workflow Executors. Defaults to `.Values.images.pullPolicy`.
      pullPolicy: ""
    # -- Resource limits and requests for the Workflow Executors
    resources: {}
    # -- Passes arguments to the executor processes
    args: []
    # -- Adds environment variables for the executor.
    env: []
    # -- sets security context for the executor container
    securityContext: {}
  
  server:
    # -- Deploy the Argo Server
    enabled: true
    # -- Value for base href in index.html. Used if the server is running behind reverse proxy under subpath different from /.
    ## only updates base url of resources on client side,
    ## it's expected that a proxy server rewrites the request URL and gets rid of this prefix
    ## https://github.com/argoproj/argo-workflows/issues/716#issuecomment-433213190
    baseHref: /
    image:
      # -- Registry to use for the server
      registry: ccr.ccs.tencentyun.com
      # -- Repository to use for the server
      repository: tke-market/argocli
      # -- Image tag for the Argo Workflows server. Defaults to `.Values.images.tag`.
      tag: ""
    # -- optional map of annotations to be applied to the ui Deployment
    deploymentAnnotations: {}
    # -- optional map of annotations to be applied to the ui Pods
    podAnnotations: {}
    # -- Optional labels to add to the UI pods
    podLabels: {}
    # -- SecurityContext to set on the server pods
    podSecurityContext: {}
    rbac:
      # -- Adds Role and RoleBinding for the server.
      create: true
    # -- Servers container-level security context
    securityContext:
      readOnlyRootFilesystem: false
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
    # -- Server name string
    name: server
    # -- Service type for server pods
    serviceType: ClusterIP
    # -- Service port for server
    servicePort: 2746
    # -- Service node port
    serviceNodePort: # 32746
    # -- Service port name
    servicePortName: "" # http
  
    # -- Mapping between IP and hostnames that will be injected as entries in the pod's hosts files
    hostAliases: []
    # - ip: 10.20.30.40
    #   hostnames:
    #   - git.myhostname
  
    serviceAccount:
      # -- Create a service account for the server
      create: true
      # -- Service account name
      name: ""
      # -- Labels applied to created service account
      labels: {}
      # -- Annotations applied to created service account
      annotations: {}
  
    # -- Annotations to be applied to the UI Service
    serviceAnnotations: {}
    # -- Optional labels to add to the UI Service
    serviceLabels: {}
    # -- The class of the load balancer implementation
    loadBalancerClass: ""
    # -- Static IP address to assign to loadBalancer service type `LoadBalancer`
    loadBalancerIP: ""
    # -- Source ranges to allow access to service from. Only applies to service type `LoadBalancer`
    loadBalancerSourceRanges: []
    # -- Resource limits and requests for the server
    resources: {}
    # -- The number of server pods to run
    replicas: 1
    # -- The number of revisions to keep.
    revisionHistoryLimit: 10
    ## Argo Server Horizontal Pod Autoscaler
    autoscaling:
      # -- Enable Horizontal Pod Autoscaler ([HPA]) for the Argo Server
      enabled: false
      # -- Minimum number of replicas for the Argo Server [HPA]
      minReplicas: 1
      # -- Maximum number of replicas for the Argo Server [HPA]
      maxReplicas: 5
      # -- Average CPU utilization percentage for the Argo Server [HPA]
      targetCPUUtilizationPercentage: 50
      # -- Average memory utilization percentage for the Argo Server [HPA]
      targetMemoryUtilizationPercentage: 50
      # -- Configures the scaling behavior of the target in both Up and Down directions.
      # This is only available on HPA apiVersion `autoscaling/v2beta2` and newer
      behavior: {}
        # scaleDown:
        #  stabilizationWindowSeconds: 300
        #  policies:
        #   - type: Pods
        #     value: 1
        #     periodSeconds: 180
        # scaleUp:
        #   stabilizationWindowSeconds: 300
        #   policies:
        #   - type: Pods
        #     value: 2
    pdb:
      # -- Configure [Pod Disruption Budget] for the server pods
      enabled: false
      # minAvailable: 1
      # maxUnavailable: 1
  
    # -- [Node selector]
    nodeSelector:
      kubernetes.io/os: linux
  
    # -- [Tolerations] for use with node taints
    tolerations: []
  
    # -- Assign custom [affinity] rules
    affinity: {}
  
    # -- Assign custom [TopologySpreadConstraints] rules to the argo server
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment
    topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
  
    # -- Leverage a PriorityClass to ensure your pods survive resource shortages
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    priorityClassName: ""
  
    # -- Run the argo server in "secure" mode. Configure this value instead of `--secure` in extraArgs.
    ## See the following documentation for more details on secure mode:
    ## https://argo-workflows.readthedocs.io/en/stable/tls/
    secure: false
  
    # -- Extra environment variables to provide to the argo-server container
    extraEnv: []
      # - name: FOO
      #   value: "bar"
  
    # -- Deprecated; use server.authModes instead.
    authMode: ""
  
    # -- A list of supported authentication modes. Available values are `server`, `client`, or `sso`. If you provide sso, please configure `.Values.server.sso` as well.
    ## Ref: https://argo-workflows.readthedocs.io/en/stable/argo-server-auth-mode/
    authModes: []
  
    # -- Extra arguments to provide to the Argo server binary.
    ## Ref: https://argo-workflows.readthedocs.io/en/stable/argo-server/#options
    extraArgs: []
  
    logging:
      # -- Set the logging level (one of: `debug`, `info`, `warn`, `error`)
      level: info
      # -- Set the glog logging level
      globallevel: "0"
      # -- Set the logging format (one of: `text`, `json`)
      format: "text"
  
    # -- Volume to be mounted in Pods for temporary files.
    tmpVolume:
      emptyDir: {}
    # -- Additional volume mounts to the server main container.
    volumeMounts: []
    # -- Additional volumes to the server pod.
    volumes: []
  
    ## Ingress configuration.
    # ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ingress:
      # -- Enable an ingress resource
      enabled: false
      # -- Additional ingress annotations
      annotations: {}
      # -- Additional ingress labels
      labels: {}
      # -- Defines which ingress controller will implement the resource
      ingressClassName: ""
  
      # -- List of ingress hosts
      ## Hostnames must be provided if Ingress is enabled.
      ## Secrets must be manually created in the namespace
      hosts: []
        # - argoworkflows.example.com
  
      # -- List of ingress paths
      paths:
        - /
  
      # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`
      pathType: Prefix
      # -- Additional ingress paths
      extraPaths: []
        # - path: /*
        #   backend:
        #     serviceName: ssl-redirect
        #     servicePort: use-annotation
        ## for Kubernetes >=1.19 (when "networking.k8s.io/v1" is used)
        # - path: /*
        #   pathType: Prefix
        #   backend:
        #     service
        #       name: ssl-redirect
        #       port:
        #         name: use-annotation
  
      # -- Ingress TLS configuration
      tls: []
        # - secretName: argoworkflows-example-tls
        #   hosts:
        #     - argoworkflows.example.com
  
    ## Create a Google Backendconfig  for use with the GKE Ingress Controller
    ## https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-configuration#configuring_ingress_features_through_backendconfig_parameters
    GKEbackendConfig:
      # -- Enable BackendConfig custom resource for Google Kubernetes Engine
      enabled: false
      # -- [BackendConfigSpec]
      spec: {}
    #  spec:
    #    iap:
    #      enabled: true
    #      oauthclientCredentials:
    #        secretName: argoworkflows-secret
  
    ## Create a Google Managed Certificate for use with the GKE Ingress Controller
    ## https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs
    GKEmanagedCertificate:
      # -- Enable ManagedCertificate custom resource for Google Kubernetes Engine.
      enabled: false
      # -- Domains for the Google Managed Certificate
      domains:
      - argoworkflows.example.com
  
    ## Create a Google FrontendConfig Custom Resource, for use with the GKE Ingress Controller
    ## https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features#configuring_ingress_features_through_frontendconfig_parameters
    GKEfrontendConfig:
      # -- Enable FrontConfig custom resource for Google Kubernetes Engine
      enabled: false
      # -- [FrontendConfigSpec]
      spec: {}
    # spec:
    #   redirectToHttps:
    #     enabled: true
    #     responseCodeName: RESPONSE_CODE
  
    clusterWorkflowTemplates:
      # -- Create a ClusterRole and CRB for the server to access ClusterWorkflowTemplates.
      enabled: true
      # -- Give the server permissions to edit ClusterWorkflowTemplates.
      enableEditing: true
  
    # SSO configuration when SSO is specified as a server auth mode.
    sso:
      # -- Create SSO configuration. If you set `true` , please also set `.Values.server.authModes` as `sso`.
      enabled: false
      # -- The root URL of the OIDC identity provider
      issuer: https://accounts.google.com
      clientId:
        # -- Name of secret to retrieve the app OIDC client ID
        name: argo-server-sso
        # -- Key of secret to retrieve the app OIDC client ID
        key: client-id
      clientSecret:
        # -- Name of a secret to retrieve the app OIDC client secret
        name: argo-server-sso
        # -- Key of a secret to retrieve the app OIDC client secret
        key: client-secret
      # -- The OIDC redirect URL. Should be in the form <argo-root-url>/oauth2/callback.
      redirectUrl: ""
      rbac:
        # -- Adds ServiceAccount Policy to server (Cluster)Role.
        enabled: true
      # -- Whitelist to allow server to fetch Secrets
      ## When present, restricts secrets the server can read to a given list.
      ## You can use it to restrict the server to only be able to access the
      ## service account token secrets that are associated with service accounts
      ## used for authorization.
        secretWhitelist: []
      # -- Scopes requested from the SSO ID provider
      ## The 'groups' scope requests group membership information, which is usually used for authorization decisions.
      scopes: []
        # - groups
      # -- Define how long your login is valid for (in hours)
      ## If omitted, defaults to 10h.
      sessionExpiry: ""
      # -- Alternate root URLs that can be included for some OIDC providers
      issuerAlias: ""
      # -- Override claim name for OIDC groups
      customGroupClaimName: ""
      # -- Specify the user info endpoint that contains the groups claim
      ## Configure this if your OIDC provider provides groups information only using the user-info endpoint (e.g. Okta)
      userInfoPath: ""
      # -- Skip TLS verification for the HTTP client
      insecureSkipVerify: false
      # -- Filter the groups returned by the OIDC provider
      ## A logical "OR" is used between each regex in the list
      filterGroupsRegex: []
        # - ".*argo-wf.*"
        # - ".*argo-workflow.*"
  
    # -- Extra containers to be added to the server deployment
    extraContainers: []
  
    # -- Enables init containers to be added to the server deployment
    extraInitContainers: []
  
    # -- Specify postStart and preStop lifecycle hooks for server container
    lifecycle: {}
  
    # -- terminationGracePeriodSeconds for container lifecycle hook
    terminationGracePeriodSeconds: 30
  
    ## livenessProbe for server
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
    livenessProbe:
      # -- Enable Kubernetes liveness probe for server
      enabled: false
      httpGet:
        # -- Http port to use for the liveness probe
        port: 2746
        # -- Http path to use for the liveness probe
        path: /
      # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded
      failureThreshold: 3
      # -- Number of seconds after the container has started before [probe] is initiated
      initialDelaySeconds: 10
      # -- How often (in seconds) to perform the [probe]
      periodSeconds: 10
      # -- Number of seconds after which the [probe] times out
      timeoutSeconds: 1
      # -- Minimum consecutive successes for the [probe] to be considered successful after having failed
      successThreshold: 1
  
  # -- Array of extra K8s manifests to deploy
  extraObjects: []
  
  # -- Use static credentials for S3 (eg. when not using AWS IRSA)
  useStaticCredentials: true
  artifactRepository:
    # -- Archive the main container logs as an artifact
    archiveLogs: false
    # -- Store artifact in a S3-compliant object store
    # @default -- See [values.yaml]
    s3: {}
    gcs: {}
    azure: {}
  
  # -- The section of custom artifact repository.
  # Utilize a custom artifact repository that is not one of the current base ones (s3, gcs, azure)
  customArtifactRepository: {}
  # artifactory:
  #   repoUrl: https://artifactory.example.com/raw
  #   usernameSecret:
  #     name: artifactory-creds
  #     key: username
  #   passwordSecret:
  #     name: artifactory-creds
  #     key: password
  
  # -- The section of [artifact repository ref](https://argo-workflows.readthedocs.io/en/stable/artifact-repository-ref/).
  # Each map key is the name of configmap
  # @default -- `{}` (See [values.yaml])
  artifactRepositoryRef: {}
  
  emissary:
    # -- The command/args for each image on workflow, needed when the command is not specified and the emissary executor is used.
    ## See more: https://argo-workflows.readthedocs.io/en/stable/workflow-executors/#emissary-emissary
    images: []
    #  argoproj/argosay:v2:
    #    cmd: [/argosay]
    #  docker/whalesay:latest:
    #    cmd: [/bin/bash]


grafana:


kube-prometheus-stack:
  ## Provide a name in place of kube-prometheus-stack for `app:` labels
  ##
  nameOverride: ""
 
  ## Override the deployment namespace
  ##
  namespaceOverride: ""
 
  ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6
  ##
  kubeTargetVersionOverride: ""
 
  ## Allow kubeVersion to be overridden while creating the ingress
  ##
  kubeVersionOverride: ""
 
  ## Provide a name to substitute for the full names of resources
  ##
  fullnameOverride: "tke-ai-prometheus"
 
  ## Labels to apply to all resources
  ##
  commonLabels: {}
  # scmhash: abc123
  # myLabel: aakkmd
 
  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: true
    ## The CRD upgrade job mitigates the limitation of helm not being able to upgrade CRDs.
    ## The job will apply the CRDs to the cluster before the operator is deployed, using helm hooks.
    ## It deploy a corresponding clusterrole, clusterrolebinding and serviceaccount to apply the CRDs.
    ## This feature is in preview, off by default and may change in the future.
    upgradeJob:
      enabled: false
  ## custom Rules to override "for" and "severity" in defaultRules
  ##
  customRules: {}
    # AlertmanagerFailedReload:
    #   for: 3m
    # AlertmanagerMembersInconsistent:
    #   for: 5m
    #   severity: "warning"
 
  ## Create default rules for monitoring the cluster
  ##
  defaultRules:
    create: false
 
    ## Reduce app namespace alert scope
    appNamespacesTarget: ".*"
 
    ## Set keep_firing_for for all alerts
    keepFiringFor: ""
 
    ## Labels for default rules
    labels: {}
    ## Annotations for default rules
    annotations: {}
 
    ## Additional labels for PrometheusRule alerts
    additionalRuleLabels: {}
 
    ## Additional annotations for PrometheusRule alerts
    additionalRuleAnnotations: {}
 
    ## Additional labels for specific PrometheusRule alert groups
    additionalRuleGroupLabels:
      alertmanager: {}
      etcd: {}
      configReloaders: {}
      general: {}
      k8sContainerCpuUsageSecondsTotal: {}
      k8sContainerMemoryCache: {}
      k8sContainerMemoryRss: {}
      k8sContainerMemorySwap: {}
      k8sContainerResource: {}
      k8sPodOwner: {}
      kubeApiserverAvailability: {}
      kubeApiserverBurnrate: {}
      kubeApiserverHistogram: {}
      kubeApiserverSlos: {}
      kubeControllerManager: {}
      kubelet: {}
      kubeProxy: {}
      kubePrometheusGeneral: {}
      kubePrometheusNodeRecording: {}
      kubernetesApps: {}
      kubernetesResources: {}
      kubernetesStorage: {}
      kubernetesSystem: {}
      kubeSchedulerAlerting: {}
      kubeSchedulerRecording: {}
      kubeStateMetrics: {}
      network: {}
      node: {}
      nodeExporterAlerting: {}
      nodeExporterRecording: {}
      prometheus: {}
      prometheusOperator: {}
 
    ## Additional annotations for specific PrometheusRule alerts groups
    additionalRuleGroupAnnotations:
      alertmanager: {}
      etcd: {}
      configReloaders: {}
      general: {}
      k8sContainerCpuUsageSecondsTotal: {}
      k8sContainerMemoryCache: {}
      k8sContainerMemoryRss: {}
      k8sContainerMemorySwap: {}
      k8sContainerResource: {}
      k8sPodOwner: {}
      kubeApiserverAvailability: {}
      kubeApiserverBurnrate: {}
      kubeApiserverHistogram: {}
      kubeApiserverSlos: {}
      kubeControllerManager: {}
      kubelet: {}
      kubeProxy: {}
      kubePrometheusGeneral: {}
      kubePrometheusNodeRecording: {}
      kubernetesApps: {}
      kubernetesResources: {}
      kubernetesStorage: {}
      kubernetesSystem: {}
      kubeSchedulerAlerting: {}
      kubeSchedulerRecording: {}
      kubeStateMetrics: {}
      network: {}
      node: {}
      nodeExporterAlerting: {}
      nodeExporterRecording: {}
      prometheus: {}
      prometheusOperator: {}
 
    additionalAggregationLabels: []
 
    ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
    runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
 
    node:
      fsSelector: 'fstype!=""'
      # fsSelector: 'fstype=~"ext[234]|btrfs|xfs|zfs"'
 
    ## Disabled PrometheusRule alerts
    disabled: {}
    # KubeAPIDown: true
    # NodeRAIDDegraded: true
 
  ## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.
  ##
  # additionalPrometheusRules: []
  #  - name: my-rule-file
  #    groups:
  #      - name: my_group
  #        rules:
  #        - record: my_record
  #          expr: 100 * my_record
 
  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  #  rule-name:
  #    groups:
  #    - name: my_group
  #      rules:
  #      - record: my_record
  #        expr: 100 * my_record
  additionalPrometheusRulesMap: 
    rule-name:
      groups:
      - name: sglang
        rules:
        - record: sglang:queue_reqs:ratio
          expr: rate(sglang:num_queue_reqs[1m])
        - record: sglang:avg_queue_latency:ratio
          expr: rate(sglang:avg_request_queue_latency[1m])
        - record: sglang:e2e_latency_p95
          expr: histogram_quantile(0.95, sum(rate(sglang:e2e_request_latency_seconds_bucket[5m])) by (le))
        - record: sglang:ttft_latency_p95
          expr: histogram_quantile(0.95, sum(rate(sglang:time_to_first_token_seconds_bucket[5m])) by (le))

  global:
    rbac:
      create: true
 
      ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
      ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
      createAggregateClusterRoles: false
      pspEnabled: false
      pspAnnotations: {}
    ## Global image registry to use if it needs to be overridden for some specific use cases (e.g local registries, custom images, ...)
    ##
    imageRegistry: ""
 
    ## Reference to one or more secrets to be used when pulling images
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    imagePullSecrets: []
    # - name: "image-pull-secret"
    # or
    # - "image-pull-secret"
 
  windowsMonitoring:
    ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
    enabled: false
 
  ## Configuration for prometheus-windows-exporter
  ## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
  ##
  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: true
        jobLabel: jobLabel
 
    releaseLabel: true
 
    ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
    ##
    podLabels:
      jobLabel: windows-exporter
 
    ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
    ##
    config: |-
      collectors:
        enabled: '[defaults],memory,container'
 
  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:
    ## Deploy alertmanager
    ##
    enabled: false
 
  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true
    namespaceOverride: ""
 
    ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
    ##
    forceDeployDatasources: false
 
    ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
    ##
    forceDeployDashboards: false
 
    ## Deploy default dashboards
    ##
    defaultDashboardsEnabled: false
 
    ## Deploy GrafanaDashboard CRDs that reference dashboards from ConfigMaps when grafana-operator is used
    ## These settings control how dashboards are integrated with the Grafana Operator
    ## Note: End user still need to create is own kind: GrafanaDataSource for Prometheus
    ## eg:
    ## apiVersion: grafana.integreatly.org/v1beta1
    ## kind: GrafanaDatasource
    ## metadata:
    ##   name: prometheus
    ##   annotations: {}
    ## spec:
    ##   allowCrossNamespaceImport: true
    ##   instanceSelector:
    ##     matchLabels:
    ##       app: grafana
    ##   datasource:
    ##     name: prometheus
    ##     type: prometheus
    ##     access: proxy
    ##     url: http://prometheus-operated.prometheus-stack.svc.cluster.local:9090
    ##     isDefault: true
    ##     jsonData:
    ##       "tlsSkipVerify": true
    ##       "timeInterval": "5s"
    ##
    operator:
      ## Enable references to ConfigMaps containing dashboards in GrafanaDashboard CRs
      ## Set to true to allow dashboards to be loaded from ConfigMap references
      dashboardsConfigMapRefEnabled: true
 
      ## Annotations for GrafanaDashboard Cr
      ##
      annotations: {}
      ## Labels that should be matched kind: Grafana instance
      ## Example: { app: grafana, category: dashboard }
      ##
      matchLabels: {}
 
      ## How frequently the operator should resync resources (in duration format)
      ## Controls how often dashboards are reconciled by the operator
      ##
      resyncPeriod: 10m
 
      ## Which folder all ddashboard in Grafana General means on Root level
      ##
      folder: General
 
    ## Timezone for the default dashboards
    ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
    ##
    defaultDashboardsTimezone: utc
 
    ## Editable flag for the default dashboards
    ##
    defaultDashboardsEditable: true
 
    ## Default interval for Grafana dashboards
    ##
    defaultDashboardsInterval: 1m
 
    adminUser: admin
    adminPassword: tke-ai
 
    rbac:
      ## If true, Grafana PSPs will be created
      ##
      pspEnabled: false
 
    ingress:
      ## If true, Grafana Ingress will be created
      ##
      enabled: false
 
      ## IngressClassName for Grafana Ingress.
      ## Should be provided if Ingress is enable.
      ##
      # ingressClassName: nginx
 
      ## Annotations for Grafana Ingress
      ##
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
 
      ## Labels to be added to the Ingress
      ##
      labels: {}
 
      ## Hostnames.
      ## Must be provided if Ingress is enable.
      ##
      # hosts:
      #   - grafana.domain.com
      hosts: []
 
      ## Path for grafana ingress
      path: /
 
      ## TLS configuration for grafana Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: grafana-general-tls
      #   hosts:
      #   - grafana.example.com
 
    # # To make Grafana persistent (Using Statefulset)
    # #
    # persistence:
    #   enabled: true
    #   type: sts
    #   storageClassName: "storageClassName"
    #   accessModes:
    #     - ReadWriteOnce
    #   size: 20Gi
    #   finalizers:
    #     - kubernetes.io/pvc-protection
 
    serviceAccount:
      create: true
      autoMount: true
 
    sidecar:
      dashboards:
        enabled: false
        label: grafana_dashboard
        labelValue: "1"
        # Allow discovery in all namespaces for dashboards
        searchNamespace: ALL
 
        # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels
        enableNewTablePanelSyntax: false
 
        ## Annotations for Grafana dashboard configmaps
        ##
        annotations: {}
        multicluster:
          global:
            enabled: false
          etcd:
            enabled: false
        provider:
          allowUiUpdates: false
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        isDefaultDatasource: true
 
        name: Prometheus
        uid: prometheus
 
        ## URL of prometheus datasource
        ##
        # url: http://prometheus-stack-prometheus:9090/
 
        ## Prometheus request timeout in seconds
        # timeout: 30
 
        # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
        # defaultDatasourceScrapeInterval: 15s
 
        ## Annotations for Grafana datasource configmaps
        ##
        annotations: {}
 
        ## Set method for HTTP to send query to datasource
        httpMethod: POST
 
        ## Create datasource for each Pod of Prometheus StatefulSet;
        ## this uses by default the headless service `prometheus-operated` which is
        ## created by Prometheus Operator. In case you deployed your own Service for your
        ## Prometheus instance, you can specifiy it with the field `prometheusServiceName`
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286
        createPrometheusReplicasDatasources: false
        prometheusServiceName: prometheus-operated
        label: grafana_datasource
        labelValue: "1"
 
        ## Field with internal link pointing to existing data source in Grafana.
        ## Can be provisioned via additionalDataSources
        exemplarTraceIdDestinations: {}
          # datasourceUid: Jaeger
          # traceIdLabelName: trace_id
          # urlDisplayLabel: View traces
        alertmanager:
          enabled: false
          ## URL of alertmanager datasource
          name: Alertmanager
          uid: alertmanager
          handleGrafanaManagedAlerts: false
          implementation: prometheus
 
    extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   configMap: certs-configmap
    #   readOnly: true
 
    dashboardProviders: 
      sglang.yaml:
        apiVersion: 1
        providers:
          - name: 'dashboards'
            orgId: 1
            folder: 'TKE AI Monitoring'
            type: file
            disableDeletion: false
            updateIntervalSeconds: 10
            allowUiUpdates: false
            options:
              path: /var/lib/grafana/dashboards
              foldersFromFilesStructure: true

    dashboardsConfigMaps:
      sglang: tke-ai-sglang-dashboard
      dcgm-exporter: tke-ai-dcgm-exporter-dashboard
      
    deleteDatasources: []
    # - name: example-datasource
    #   orgId: 1
 
    ## Configure additional grafana datasources (passed through tpl)
    ## ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#datasources
    additionalDataSources: []
    # - name: prometheus-sample
    #   access: proxy
    #   basicAuth: true
    #   secureJsonData:
    #       basicAuthPassword: pass
    #   basicAuthUser: daco
    #   editable: false
    #   jsonData:
    #       tlsSkipVerify: true
    #   orgId: 1
    #   type: prometheus
    #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
    #   version: 1
 
    # Flag to mark provisioned data sources for deletion if they are no longer configured.
    # It takes no effect if data sources are already listed in the deleteDatasources section.
    # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-configuration-file
    prune: false
 
    ## Passed to grafana subchart and used by servicemonitor below
    ##
    service:
      portName: http-web
      ipFamilies: []
      ipFamilyPolicy: ""
 
    serviceMonitor:
      # If true, a ServiceMonitor CRD is created for a prometheus operator
      # https://github.com/prometheus-operator/prometheus-operator
      #
      enabled: false
 
      # Path to use for scraping metrics. Might be different if server.root_url is set
      # in grafana.ini
      path: "/metrics"
 
      #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)
 
      # labels for the ServiceMonitor
      labels: {}
 
      # Scrape interval. If not set, the Prometheus default scrape interval is used.
      #
      interval: ""
      scheme: http
      tlsConfig: {}
      scrapeTimeout: 30s
 
      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
  ## Flag to disable all the kubernetes component scrapers
  ##
  kubernetesServiceMonitors:
    enabled: false
 
  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: false
 
  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: false
    namespace: kube-system
 
    serviceMonitor:
      enabled: true
      ## Enable scraping /metrics from kubelet's service
      kubelet: true
 
      ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.
      ##
      attachMetadata:
        node: false
 
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
 
      ## If true, Prometheus use (respect) labels provided by exporter.
      ##
      honorLabels: true
 
      ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
      ##
      honorTimestamps: true
 
      ## If true, defines whether Prometheus tracks staleness of the metrics that have an explicit timestamp present in scraped data. Has no effect if `honorTimestamps` is false.
      ## We recommend enabling this if you want the best possible accuracy for container_ metrics scraped from cadvisor.
      ## For more details see: https://github.com/prometheus-community/helm-charts/pull/5063#issuecomment-2545374849
      trackTimestampsStaleness: true
 
      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0
 
      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0
 
      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0
 
      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0
 
      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0
 
      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
 
      ## Enable scraping the kubelet over https. For requirements to enable this see
      ## https://github.com/prometheus-operator/prometheus-operator/issues/926
      ##
      https: true
 
      ## Skip TLS certificate validation when scraping.
      ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed
      ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs
      ##
      insecureSkipVerify: true
 
      ## Enable scraping /metrics/probes from kubelet's service
      ##
      probes: true
 
      ## Enable scraping /metrics/resource from kubelet's service
      ## This is disabled by default because container metrics are already exposed by cAdvisor
      ##
      resource: false
      # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
      resourcePath: "/metrics/resource/v1alpha1"
      ## Configure the scrape interval for resource metrics. This is configured to the default Kubelet cAdvisor
      ## minimum housekeeping interval in order to avoid missing samples. Note, this value is ignored
      ## if kubelet.serviceMonitor.interval is not empty.
      resourceInterval: 10s
 
      ## Enable scraping /metrics/cadvisor from kubelet's service
      ##
      cAdvisor: true
      ## Configure the scrape interval for cAdvisor. This is configured to the default Kubelet cAdvisor
      ## minimum housekeeping interval in order to avoid missing samples. Note, this value is ignored
      ## if kubelet.serviceMonitor.interval is not empty.
      cAdvisorInterval: 10s
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      cAdvisorMetricRelabelings:
        # Drop less useful container CPU metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
        # Drop less useful container / always zero filesystem metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
        # Drop less useful / always zero container memory metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_memory_(mapped_file|swap)'
        # Drop less useful container process metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(file_descriptors|tasks_state|threads_max)'
        # Drop container_memory_failures_total{scope="hierarchy"} metrics,
        # we only need the container scope.
        - sourceLabels: [__name__, scope]
          action: drop
          regex: 'container_memory_failures_total;hierarchy'
        # Drop container_network_... metrics that match various interfaces that
        # correspond to CNI and similar interfaces. This avoids capturing network
        # metrics for host network containers.
        - sourceLabels: [__name__, interface]
          action: drop
          regex: 'container_network_.*;(cali|cilium|cni|lxc|nodelocaldns|tunl).*'
        # Drop container spec metrics that overlap with kube-state-metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_spec.*'
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: '.+;'
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop
 
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      probesMetricRelabelings: []
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop
 
      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      ## metrics_path is required to match upstream rules and charts
      cAdvisorRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      probesRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      resourceRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings:
        # Reduce bucket cardinality of kubelet storage operations.
        - action: drop
          sourceLabels: [__name__, le]
          regex: (csi_operations|storage_operation_duration)_seconds_bucket;(0.25|2.5|15|25|120|600)(\.0)?
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop
 
      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      ## metrics_path is required to match upstream rules and charts
      relabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar
 
      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []
 
  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: false
 
  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: false
  ## Component scraping kubeDns. Use either this or coreDns
  ##
  kubeDns:
    enabled: false
 
  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false
 
  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false
 
  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: false
 
  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true
 
  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    namespaceOverride: ""
    rbac:
      create: true
    releaseLabel: true
 
    ## Enable scraping via kubernetes-service-endpoints
    ## Disabled by default as we service monitor is enabled below
    ##
    prometheusScrape: false
 
    prometheus:
      monitor:
 
        ## Enable scraping via service monitor
        ## Disable to prevent duplication if you enable prometheusScrape above
        ##
        enabled: true
 
        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""
 
        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0
 
        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0
 
        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0
 
        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0
 
        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0
 
        ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
        ##
        scrapeTimeout: ""
 
        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""
 
        # Keep labels from scraped data, overriding server-side labels
        ##
        honorLabels: true
 
        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]
 
        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
 
    selfMonitor:
      enabled: false
 
  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: false
    operatingSystems:
      linux:
        enabled: true
      aix:
        enabled: true
      darwin:
        enabled: true
 
    ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
    ##
    forceDeployDashboards: false
 
  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    namespaceOverride: ""
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    releaseLabel: true
    extraArgs:
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    service:
      portName: http-metrics
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      labels:
        jobLabel: node-exporter
 
    prometheus:
      monitor:
        enabled: true
 
        jobLabel: jobLabel
 
        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""
 
        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0
 
        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0
 
        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0
 
        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0
 
        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0
 
        ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
        ##
        scrapeTimeout: ""
 
        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""
 
        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - sourceLabels: [__name__]
        #   separator: ;
        #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
        #   replacement: $1
        #   action: drop
 
        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
 
        ## Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above.
        ##
        # attachMetadata:
        #   node: false
 
    rbac:
      ## If true, create PSPs for node-exporter
      ##
      pspEnabled: false
 
  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true
 
    ## Use '{{ template "kube-prometheus-stack.fullname" . }}-operator' by default
    fullnameOverride: ""
 
    ## Number of old replicasets to retain ##
    ## The default value is 10, 0 will garbage-collect old replicasets ##
    revisionHistoryLimit: 10
 
    ## Strategy of the deployment
    ##
    strategy: {}
 
    ## Prometheus-Operator v0.39.0 and later support TLS natively.
    ##
    tls:
      enabled: true
      # Value must match version names from https://pkg.go.dev/crypto/tls#pkg-constants
      tlsMinVersion: VersionTLS13
      # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
      internalPort: 10250
 
    ## Liveness probe for the prometheusOperator deployment
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 0
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## Readiness probe for the prometheusOperator deployment
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 0
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
 
    ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
    ## rules from making their way into prometheus and potentially preventing the container from starting
    admissionWebhooks:
      ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
      ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
      failurePolicy: ""
      ## The default timeoutSeconds is 10 and the maximum value is 30.
      timeoutSeconds: 10
      enabled: true
      ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
      ## If unspecified, system trust roots on the apiserver are used.
      caBundle: ""
      ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
      ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
      ## certs ahead of time if you wish.
      ##
      annotations: {}
      #   argocd.argoproj.io/hook: PreSync
      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
 
      namespaceSelector: {}
      objectSelector: {}
 
      mutatingWebhookConfiguration:
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync
 
      validatingWebhookConfiguration:
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync
 
      deployment:
        enabled: false
 
        ## Number of replicas
        ##
        replicas: 1
 
        ## Strategy of the deployment
        ##
        strategy: {}
 
        # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
        podDisruptionBudget:
          enabled: false
          minAvailable: 1
          # maxUnavailable: ""
          unhealthyPodEvictionPolicy: AlwaysAllow
 
        ## Number of old replicasets to retain ##
        ## The default value is 10, 0 will garbage-collect old replicasets ##
        revisionHistoryLimit: 10
 
        ## Prometheus-Operator v0.39.0 and later support TLS natively.
        ##
        tls:
          enabled: true
          # Value must match version names from https://pkg.go.dev/crypto/tls#pkg-constants
          tlsMinVersion: VersionTLS13
          # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
          internalPort: 10250
 
        ## Service account for Prometheus Operator Webhook to use.
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
        ##
        serviceAccount:
          annotations: {}
          automountServiceAccountToken: false
          create: true
          name: ""
 
        ## Configuration for Prometheus operator Webhook service
        ##
        service:
          annotations: {}
          labels: {}
          clusterIP: ""
          ipDualStack:
            enabled: false
            ipFamilies: ["IPv6", "IPv4"]
            ipFamilyPolicy: "PreferDualStack"
 
          ## Port to expose on each node
          ## Only used if service.type is 'NodePort'
          ##
          nodePort: 31080
 
          nodePortTls: 31443
 
          ## Additional ports to open for Prometheus operator Webhook service
          ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
          ##
          additionalPorts: []
 
          ## Loadbalancer IP
          ## Only use if service.type is "LoadBalancer"
          ##
          loadBalancerIP: ""
          loadBalancerSourceRanges: []
 
          ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
          ##
          externalTrafficPolicy: Cluster
 
          ## Service type
          ## NodePort, ClusterIP, LoadBalancer
          ##
          type: ClusterIP
 
          ## List of IP addresses at which the Prometheus server service is available
          ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
          ##
          externalIPs: []
 
        # ## Labels to add to the operator webhook deployment
        # ##
        labels: {}
 
        ## Annotations to add to the operator webhook deployment
        ##
        annotations: {}
 
        ## Labels to add to the operator webhook pod
        ##
        podLabels: {}
 
        ## Annotations to add to the operator webhook pod
        ##
        podAnnotations: {}
 
        ## Assign a PriorityClassName to pods if set
        # priorityClassName: ""
 
        ## Define Log Format
        # Use logfmt (default) or json logging
        # logFormat: logfmt
 
        ## Decrease log verbosity to errors only
        # logLevel: error
 
        ## Prometheus-operator webhook image
        ##
        image:
          registry: quay.io
          repository: prometheus-operator/admission-webhook
          # if not set appVersion field from Chart.yaml is used
          tag: ""
          sha: ""
          pullPolicy: IfNotPresent
 
        ## Define Log Format
        # Use logfmt (default) or json logging
        # logFormat: logfmt
 
        ## Decrease log verbosity to errors only
        # logLevel: error
 
 
        ## Liveness probe
        ##
        livenessProbe:
          enabled: true
          failureThreshold: 3
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
 
        ## Readiness probe
        ##
        readinessProbe:
          enabled: true
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
 
        ## Resource limits & requests
        ##
        resources: {}
        # limits:
        #   cpu: 200m
        #   memory: 200Mi
        # requests:
        #   cpu: 100m
        #   memory: 100Mi
 
        # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
        # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
        ##
        hostNetwork: false
 
        ## Define which Nodes the Pods are scheduled on.
        ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
        ##
        nodeSelector: {}
 
        ## Tolerations for use with node taints
        ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
        ##
        tolerations: []
        # - key: "key"
        #   operator: "Equal"
        #   value: "value"
        #   effect: "NoSchedule"
 
        ## Assign custom affinity rules to the prometheus operator
        ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
        ##
        affinity: {}
          # nodeAffinity:
          #   requiredDuringSchedulingIgnoredDuringExecution:
          #     nodeSelectorTerms:
          #     - matchExpressions:
          #       - key: kubernetes.io/e2e-az-name
          #         operator: In
          #         values:
          #         - e2e-az1
        #         - e2e-az2
        dnsConfig: {}
          # nameservers:
          #   - 1.2.3.4
          # searches:
          #   - ns1.svc.cluster-domain.example
          #   - my.dns.search.suffix
          # options:
          #   - name: ndots
          #     value: "2"
          #   - name: edns0
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
 
        ## Container-specific security context configuration
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        ##
        containerSecurityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
 
        ## If false then the user will opt out of automounting API credentials.
        ##
        automountServiceAccountToken: true
 
      patch:
        enabled: true
        image:
          registry: ccr.ccs.tencentyun.com
          repository: halewang/kube-webhook-certgen
          tag: v1.5.3  # latest tag: https://github.com/kubernetes/ingress-nginx/blob/main/images/kube-webhook-certgen/TAG
          sha: ""
          pullPolicy: IfNotPresent
        resources: {}
        ## Provide a priority class name to the webhook patching job
        ##
        priorityClassName: ""
        ttlSecondsAfterFinished: 60
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync
        #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
        podAnnotations: {}
        nodeSelector: {}
        affinity: {}
        tolerations: []
 
        ## SecurityContext holds pod-level security attributes and common container settings.
        ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        ##
        securityContext:
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 2000
          seccompProfile:
            type: RuntimeDefault
        ## Service account for Prometheus Operator Webhook Job Patch to use.
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
        ##
        serviceAccount:
          create: true
          annotations: {}
          automountServiceAccountToken: true
 
      # Security context for create job container
      createSecretJob:
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
 
        # Security context for patch job container
      patchWebhookJob:
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
 
      # Use certmanager to generate webhook certs
      certManager:
        enabled: false
        # self-signed root certificate
        rootCert:
          duration: ""  # default to be 5y
          # -- Set the revisionHistoryLimit on the Certificate. See
          # https://cert-manager.io/docs/reference/api-docs/#cert-manager.io/v1.CertificateSpec
          # Defaults to nil.
          revisionHistoryLimit:
        admissionCert:
          duration: ""  # default to be 1y
          # -- Set the revisionHistoryLimit on the Certificate. See
          # https://cert-manager.io/docs/reference/api-docs/#cert-manager.io/v1.CertificateSpec
          # Defaults to nil.
          revisionHistoryLimit:
        # issuerRef:
        #   name: "issuer"
        #   kind: "ClusterIssuer"
 
    ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
    ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
    ##
    namespaces: {}
      # releaseNamespace: true
      # additional:
      # - kube-system
 
    ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
    ##
    denyNamespaces: []
 
    ## Filter namespaces to look for prometheus-operator custom resources
    ##
    alertmanagerInstanceNamespaces: []
    alertmanagerConfigNamespaces: []
    prometheusInstanceNamespaces: []
    thanosRulerInstanceNamespaces: []
 
    ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
    ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
    ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
    ##
    # clusterDomain: "cluster.local"
 
    networkPolicy:
      ## Enable creation of NetworkPolicy resources.
      ##
      enabled: false
 
      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes
 
      # cilium:
      #   egress:
 
      ## match labels used in selector
      # matchLabels: {}
 
    ## Service account for Prometheus Operator to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      automountServiceAccountToken: true
      annotations: {}
 
    # -- terminationGracePeriodSeconds for container lifecycle hook
    terminationGracePeriodSeconds: 30
    # -- Specify lifecycle hooks for the  controller
    lifecycle: {}
    ## Configuration for Prometheus operator service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
 
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
      nodePort: 30080
 
      nodePortTls: 30443
 
    ## Additional ports to open for Prometheus operator service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
    ##
      additionalPorts: []
 
    ## Loadbalancer IP
    ## Only use if service.type is "LoadBalancer"
    ##
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
 
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
 
    ## Service type
    ## NodePort, ClusterIP, LoadBalancer
    ##
      type: ClusterIP
 
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
      ##
      externalIPs: []
 
    # ## Labels to add to the operator deployment
    # ##
    labels: {}
 
    ## Annotations to add to the operator deployment
    ##
    annotations: {}
 
    ## Labels to add to the operator pod
    ##
    podLabels: {}
 
    ## Annotations to add to the operator pod
    ##
    podAnnotations: {}
 
    ## Assign a podDisruptionBudget to the operator
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      # maxUnavailable: ""
      unhealthyPodEvictionPolicy: AlwaysAllow
 
    ## Assign a PriorityClassName to pods if set
    # priorityClassName: ""
 
    ## Define Log Format
    # Use logfmt (default) or json logging
    # logFormat: logfmt
 
    ## Decrease log verbosity to errors only
    # logLevel: error
 
    kubeletService:
      ## If true, the operator will create and maintain a service for scraping kubelets
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
      ##
      enabled: true
      namespace: kube-system
      selector: ""
      ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
      name: ""
 
    ## Create Endpoints objects for kubelet targets.
    kubeletEndpointsEnabled: true
    ## Create EndpointSlice objects for kubelet targets.
    kubeletEndpointSliceEnabled: false
 
    ## Extra arguments to pass to prometheusOperator
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/operator.md
    extraArgs: []
    #  - --labels="cluster=talos-cluster"
 
    ## Create a servicemonitor for the operator
    ##
    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus operator
      ##
      selfMonitor: true
 
      ## Labels for ServiceMonitor
      additionalLabels: {}
 
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
 
      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0
 
      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0
 
      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0
 
      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0
 
      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0
 
      ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
      scrapeTimeout: ""
 
      ## Metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]
 
      #   relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
    ## Resource limits & requests
    ##
    resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 200Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi
 
    ## Operator Environment
    ##  env:
    ##    VARIABLE: value
    env:
      GOGC: "30"
 
    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
    ##
    hostNetwork: false
 
    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    ##
    nodeSelector: {}
 
    ## Tolerations for use with node taints
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    ##
    tolerations: []
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"
 
    ## Assign custom affinity rules to the prometheus operator
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2
    dnsConfig: {}
      # nameservers:
      #   - 1.2.3.4
      # searches:
      #   - ns1.svc.cluster-domain.example
      #   - my.dns.search.suffix
      # options:
      #   - name: ndots
      #     value: "2"
    #   - name: edns0
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
 
    ## Container-specific security context configuration
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ##
    containerSecurityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
 
    # Enable vertical pod autoscaler support for prometheus-operator
    verticalPodAutoscaler:
      enabled: false
 
      # Recommender responsible for generating recommendation for the object.
      # List should be empty (then the default recommender will generate the recommendation)
      # or contain exactly one recommender.
      # recommenders:
      # - name: custom-recommender-performance
 
      # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      controlledResources: []
      # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.
      # controlledValues: RequestsAndLimits
 
      # Define the max allowed resources for the pod
      maxAllowed: {}
      # cpu: 200m
      # memory: 100Mi
      # Define the min allowed resources for the pod
      minAllowed: {}
      # cpu: 200m
      # memory: 100Mi
 
      updatePolicy:
        # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
        # minReplicas: 1
        # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
        # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
        updateMode: Auto
 
    ## Prometheus-operator image
    ##
    image:
      registry: quay.io
      repository: prometheus-operator/prometheus-operator
      # if not set appVersion field from Chart.yaml is used
      tag: ""
      sha: ""
      pullPolicy: IfNotPresent
 
    ## Prometheus image to use for prometheuses managed by the operator
    ##
    # prometheusDefaultBaseImage: prometheus/prometheus
 
    ## Prometheus image registry to use for prometheuses managed by the operator
    ##
    # prometheusDefaultBaseImageRegistry: quay.io
 
    ## Alertmanager image to use for alertmanagers managed by the operator
    ##
    # alertmanagerDefaultBaseImage: prometheus/alertmanager
 
    ## Alertmanager image registry to use for alertmanagers managed by the operator
    ##
    # alertmanagerDefaultBaseImageRegistry: quay.io
 
    ## Prometheus-config-reloader
    ##
    prometheusConfigReloader:
      image:
        registry: quay.io
        repository: prometheus-operator/prometheus-config-reloader
        # if not set appVersion field from Chart.yaml is used
        tag: ""
        sha: ""
 
      # add prometheus config reloader liveness and readiness probe. Default: false
      enableProbe: false
 
      # resource config for prometheusConfigReloader
      resources: {}
        # requests:
        #   cpu: 200m
        #   memory: 50Mi
        # limits:
        #   cpu: 200m
        #   memory: 50Mi
 
    ## Thanos side-car image when configured
    ##
    thanosImage:
      registry: quay.io
      repository: thanos/thanos
      tag: v0.38.0
      sha: ""
 
    ## Set a Label Selector to filter watched prometheus and prometheusAgent
    ##
    prometheusInstanceSelector: ""
 
    ## Set a Label Selector to filter watched alertmanager
    ##
    alertmanagerInstanceSelector: ""
 
    ## Set a Label Selector to filter watched thanosRuler
    thanosRulerInstanceSelector: ""
 
    ## Set a Field Selector to filter watched secrets
    ##
    secretFieldSelector: "type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1"
 
    ## If false then the user will opt out of automounting API credentials.
    ##
    automountServiceAccountToken: true
 
    ## Additional volumes
    ##
    extraVolumes: []
 
    ## Additional volume mounts
    ##
    extraVolumeMounts: []
 
  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true
 
    ## Toggle prometheus into agent mode
    ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/prometheus-agent.md
    ##
    agentMode: false
 
    ## Annotations for Prometheus
    ##
    annotations: {}
 
    ## Configure network policy for the prometheus
    networkPolicy:
      enabled: false
 
      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes
 
      # cilium:
      #   endpointSelector:
      #   egress:
      #   ingress:
 
      # egress:
      # - {}
      # ingress:
      # - {}
      # podSelector:
      #   matchLabels:
      #     app: prometheus
 
    ## Service account for Prometheuses to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: true
 
    # Service for thanos service discovery on sidecar
    # Enable this can make Thanos Query can use
    # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
    # Thanos sidecar on prometheus nodes
    # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
    thanosService:
      enabled: false
      annotations: {}
      labels: {}
 
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
 
      ## Service type
      ##
      type: ClusterIP
 
      ## Service dual stack
      ##
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
 
      ## gRPC port config
      portName: grpc
      port: 10901
      targetPort: "grpc"
 
      ## HTTP port config (for metrics)
      httpPortName: http
      httpPort: 10902
      targetHttpPort: "http"
 
      ## ClusterIP to assign
      # Default is to make this a headless service ("None")
      clusterIP: "None"
 
      ## Port to expose on each node, if service type is NodePort
      ##
      nodePort: 30901
      httpNodePort: 30902
 
    # ServiceMonitor to scrape Sidecar metrics
    # Needs thanosService to be enabled as well
    thanosServiceMonitor:
      enabled: false
      interval: ""
 
      ## Additional labels
      ##
      additionalLabels: {}
 
      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""
 
      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: {}
 
      bearerTokenFile:
 
      ## Metric relabel configs to apply to samples before ingestion.
      metricRelabelings: []
 
      ## relabel configs to apply to samples before ingestion.
      relabelings: []
 
      ## Set default scrapeProtocols for Prometheus instances
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#scrapeprotocolstring-alias
      scrapeProtocols: []
    # Service for external access to sidecar
    # Enabling this creates a service to expose thanos-sidecar outside the cluster.
    thanosServiceExternal:
      enabled: false
      annotations: {}
      labels: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
 
      ## gRPC port config
      portName: grpc
      port: 10901
      targetPort: "grpc"
 
      ## HTTP port config (for metrics)
      httpPortName: http
      httpPort: 10902
      targetHttpPort: "http"
 
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
 
      ## Service type
      ##
      type: LoadBalancer
 
      ## Port to expose on each node
      ##
      nodePort: 30901
      httpNodePort: 30902
 
    ## Configuration for Prometheus service
    ##
    service:
      enabled: true
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
 
      ## Port for Prometheus Service to listen on
      ##
      port: 9090
 
      ## To be used with a proxy extraContainer port
      targetPort: 9090
 
      ## Port for Prometheus Reloader to listen on
      ##
      reloaderWebPort: 8080
 
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
      ##
      externalIPs: []
 
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30090
 
      ## Loadbalancer IP
      ## Only use if service.type is "LoadBalancer"
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
 
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
 
      ## Service type
      ##
      type: ClusterIP
 
      ## Additional ports to open for Prometheus service
      ##
      additionalPorts: []
      # additionalPorts:
      # - name: oauth-proxy
      #   port: 8081
      #   targetPort: 8081
      # - name: oauth-metrics
      #   port: 8082
      #   targetPort: 8082
 
      ## Consider that all endpoints are considered "ready" even if the Pods themselves are not
      ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec
      publishNotReadyAddresses: false
 
      ## If you want to make sure that connections from a particular client are passed to the same Pod each time
      ## Accepts 'ClientIP' or 'None'
      ##
      sessionAffinity: None
 
      ## If you want to modify the ClientIP sessionAffinity timeout
      ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
      ##
      sessionAffinityConfig:
        clientIP:
          timeoutSeconds: 10800
 
    ## Configuration for creating a separate Service for each statefulset Prometheus replica
    ##
    servicePerReplica:
      enabled: false
      annotations: {}
 
      ## Port for Prometheus Service per replica to listen on
      ##
      port: 9090
 
      ## To be used with a proxy extraContainer port
      targetPort: 9090
 
      ## Port to expose on each node
      ## Only used if servicePerReplica.type is 'NodePort'
      ##
      nodePort: 30091
 
      ## Loadbalancer source IP ranges
      ## Only used if servicePerReplica.type is "LoadBalancer"
      loadBalancerSourceRanges: []
 
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
 
      ## Service type
      ##
      type: ClusterIP
 
      ## Service dual stack
      ##
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
 
    ## Configure pod disruption budgets for Prometheus
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      # maxUnavailable: ""
      unhealthyPodEvictionPolicy: AlwaysAllow
 
    # Ingress exposes thanos sidecar outside the cluster
    thanosIngress:
      enabled: false
 
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
 
      annotations: {}
      labels: {}
      servicePort: 10901
 
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30901
 
      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts: []
        # - thanos-gateway.domain.com
 
      ## Paths to use for ingress rules
      ##
      paths: []
      # - /
 
      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific
 
      ## TLS configuration for Thanos Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: thanos-gateway-tls
      #   hosts:
      #   - thanos-gateway.domain.com
      #
 
    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.
 
    ingress:
      enabled: false
 
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
 
      annotations: {}
      labels: {}
 
      ## Redirect ingress to an additional defined port on the service
      # servicePort: 8081
 
      ## Hostnames.
      ## Must be provided if Ingress is enabled.
      ##
      # hosts:
      #   - prometheus.domain.com
      hosts: []
 
      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
      ##
      paths: []
      # - /
 
      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific
 
      ## TLS configuration for Prometheus Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
        # - secretName: prometheus-general-tls
        #   hosts:
        #     - prometheus.example.com
 
    # -- BETA: Configure the gateway routes for the chart here.
    # More routes can be added by adding a dictionary key like the 'main' route.
    # Be aware that this is an early beta of this feature,
    # kube-prometheus-stack does not guarantee this works and is subject to change.
    # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk
    # [[ref]](https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io%2fv1alpha2)
    route:
      main:
        # -- Enables or disables the route
        enabled: false
 
        # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2
        apiVersion: gateway.networking.k8s.io/v1
        # -- Set the route kind
        # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute
        kind: HTTPRoute
 
        annotations: {}
        labels: {}
 
        hostnames: []
        # - my-filter.example.com
        parentRefs: []
        # - name: acme-gw
 
        # -- create http route for redirect (https://gateway-api.sigs.k8s.io/guides/http-redirect-rewrite/#http-to-https-redirects)
        ## Take care that you only enable this on the http listener of the gateway to avoid an infinite redirect.
        ## matches, filters and additionalRules will be ignored if this is set to true. Be are
        httpsRedirect: false
 
        matches:
          - path:
              type: PathPrefix
              value: /
 
        ## Filters define the filters that are applied to requests that match this rule.
        filters: []
 
        ## Additional custom rules that can be added to the route
        additionalRules: []
 
    ## Configuration for creating an Ingress that will map to each Prometheus replica service
    ## prometheus.servicePerReplica must be enabled
    ##
    ingressPerReplica:
      enabled: false
 
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
 
      annotations: {}
      labels: {}
 
      ## Final form of the hostname for each per replica ingress is
      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
      ##
      ## Prefix for the per replica ingress that will have `-$replicaNumber`
      ## appended to the end
      hostPrefix: ""
      ## Domain that will be used for the per replica ingress
      hostDomain: ""
 
      ## Paths to use for ingress rules
      ##
      paths: []
      # - /
 
      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific
 
      ## Secret name containing the TLS certificate for Prometheus per replica ingress
      ## Secret must be manually created in the namespace
      tlsSecretName: ""
 
      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
      ##
      tlsSecretPerReplica:
        enabled: false
        ## Final form of the secret for each per replica ingress is
        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
        ##
        prefix: "prometheus"
 
    ## Configure additional options for default pod security policy for Prometheus
    ## ref: https://kubernetes.io/docs/concepts/security/pod-security-policy/
    podSecurityPolicy:
      allowedCapabilities: []
      allowedHostPaths: []
      volumes: []
 
    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus
      ##
      selfMonitor: true
 
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
 
      ## Additional labels
      ##
      additionalLabels: {}
 
      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0
 
      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0
 
      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0
 
      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0
 
      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0
 
      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""
 
      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: {}
 
      bearerTokenFile:
 
      ## Metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]
 
      #   relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics
 
    ## Settings affecting prometheusSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#prometheusspec
    ##
    prometheusSpec:
      ## Statefulset's persistent volume claim retention policy
      ## whenDeleted and whenScaled determine whether
      ## statefulset's PVCs are deleted (true) or retained (false)
      ## on scaling down and deleting statefulset, respectively.
      ## Requires Kubernetes version 1.27.0+.
      ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
      persistentVolumeClaimRetentionPolicy: {}
      #  whenDeleted: Retain
      #  whenScaled: Retain
 
      ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
      ##
      disableCompaction: false
 
      ## AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in the pod,
      ## If the field isn't set, the operator mounts the service account token by default.
      ## Warning: be aware that by default, Prometheus requires the service account token for Kubernetes service discovery,
      ## It is possible to use strategic merge patch to project the service account token into the 'prometheus' container.
      automountServiceAccountToken: true
 
      ## APIServerConfig
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#apiserverconfig
      ##
      apiserverConfig: {}
 
      ## Allows setting additional arguments for the Prometheus container
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#monitoring.coreos.com/v1.Prometheus
      additionalArgs: []
 
      ## File to which scrape failures are logged.
      ## Reloading the configuration will reopen the file.
      ## Defaults to empty (disabled)
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#monitoring.coreos.com/v1.Prometheus
      ##
      scrapeFailureLogFile: ""
 
      ## Interval between consecutive scrapes.
      ## Defaults to 30s.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
      ##
      scrapeInterval: ""
 
      ## Number of seconds to wait for target to respond before erroring
      ##
      scrapeTimeout: ""
 
      ## List of scrape classes to expose to scraping objects such as
      ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.
      ##
      scrapeClasses: []
      # - name: istio-mtls
      #   default: false
      #   tlsConfig:
      #     caFile: /etc/prometheus/secrets/istio.default/root-cert.pem
      #     certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem
 
      ## PodTargetLabels are appended to the `spec.podTargetLabels` field of all PodMonitor and ServiceMonitor objects.
      ##
      podTargetLabels: []
      # - customlabel
 
      ## Interval between consecutive evaluations.
      ##
      evaluationInterval: ""
 
      ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
      ##
      listenLocal: false
 
      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
      ## This is disabled by default.
      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
      ##
      enableAdminAPI: false
 
      ## Sets version of Prometheus overriding the Prometheus version as derived
      ## from the image tag. Useful in cases where the tag does not follow semver v2.
      version: ""
 
      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#webtlsconfig
      web: {}
 
      ## Exemplars related settings that are runtime reloadable.
      ## It requires to enable the exemplar storage feature to be effective.
      exemplars: {}
        ## Maximum number of exemplars stored in memory for all series.
        ## If not set, Prometheus uses its default value.
        ## A value of zero or less than zero disables the storage.
        # maxSize: 100000
 
      # EnableFeatures API enables access to Prometheus disabled features.
      # ref: https://prometheus.io/docs/prometheus/latest/feature_flags/
      enableFeatures: []
      # - exemplar-storage
 
      ##
      serviceName:
 
      ## Image of Prometheus.
      ##
      image:
        registry: quay.io
        repository: prometheus/prometheus
        tag: v3.4.1
        sha: ""
 
      ## Tolerations for use with node taints
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
      ##
      tolerations: []
      #  - key: "key"
      #    operator: "Equal"
      #    value: "value"
      #    effect: "NoSchedule"
 
      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: prometheus
 
      ## Alertmanagers to which alerts will be sent
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#alertmanagerendpoints
      ##
      ## Default configuration will connect to the alertmanager deployed as part of this release
      ##
      alertingEndpoints: []
      # - name: ""
      #   namespace: ""
      #   port: http
      #   scheme: http
      #   pathPrefix: ""
      #   tlsConfig: {}
      #   bearerTokenFile: ""
      #   apiVersion: v2
 
      ## External labels to add to any time series or alerts when communicating with external systems
      ##
      externalLabels: {}
 
      ## enable --web.enable-remote-write-receiver flag on prometheus-server
      ##
      enableRemoteWriteReceiver: false
 
      ## Name of the external label used to denote replica name
      ##
      replicaExternalLabelName: ""
 
      ## If true, the Operator won't add the external label used to denote replica name
      ##
      replicaExternalLabelNameClear: false
 
      ## Name of the external label used to denote Prometheus instance name
      ##
      prometheusExternalLabelName: ""
 
      ## If true, the Operator won't add the external label used to denote Prometheus instance name
      ##
      prometheusExternalLabelNameClear: false
 
      ## External URL at which Prometheus will be reachable.
      ##
      externalUrl: ""
 
      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
      ##
      nodeSelector: {}
 
      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
      ## with the new list of secrets.
      ##
      secrets: []
 
      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
      ##
      configMaps: []
 
      ## QuerySpec defines the query command line flags when starting Prometheus.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#queryspec
      ##
      query: {}
 
      ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.
      ruleNamespaceSelector: {}
      ## Example which selects PrometheusRules in namespaces with label "prometheus" set to "somelabel"
      # ruleNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ##
      ruleSelectorNilUsesHelmValues: true
 
      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector: {}
      ## Example which select all PrometheusRules resources
      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
      # ruleSelector:
      #   matchExpressions:
      #     - key: prometheus
      #       operator: In
      #       values:
      #         - example-rules
      #         - example-rules-2
      #
      ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
      # ruleSelector:
      #   matchLabels:
      #     role: example-rules
 
      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the servicemonitors created
      ##
      serviceMonitorSelectorNilUsesHelmValues: false
 
      ## ServiceMonitors to be selected for target discovery.
      ## If {}, select all ServiceMonitors
      ##
      serviceMonitorSelector: {}
      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
      # serviceMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## Namespaces to be selected for ServiceMonitor discovery.
      ##
      serviceMonitorNamespaceSelector: {}
      ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
      # serviceMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the podmonitors created
      ##
      podMonitorSelectorNilUsesHelmValues: false
 
      ## PodMonitors to be selected for target discovery.
      ## If {}, select all PodMonitors
      ##
      podMonitorSelector: {}
      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
      # podMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.
      podMonitorNamespaceSelector: {}
      ## Example which selects PodMonitor in namespaces with label "prometheus" set to "somelabel"
      # podMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the probes created
      ##
      probeSelectorNilUsesHelmValues: true
 
      ## Probes to be selected for target discovery.
      ## If {}, select all Probes
      ##
      probeSelector: {}
      ## Example which selects Probes with label "prometheus" set to "somelabel"
      # probeSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If nil, select own namespace. Namespaces to be selected for Probe discovery.
      probeNamespaceSelector: {}
      ## Example which selects Probe in namespaces with label "prometheus" set to "somelabel"
      # probeNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the scrapeConfigs created
      ##
      ## If null and scrapeConfigSelector is also null, exclude field from the prometheusSpec
      ## (keeping downward compatibility with older versions of CRD)
      ##
      scrapeConfigSelectorNilUsesHelmValues: true
 
      ## scrapeConfigs to be selected for target discovery.
      ## If {}, select all scrapeConfigs
      ##
      scrapeConfigSelector: {}
      ## Example which selects scrapeConfigs with label "prometheus" set to "somelabel"
      # scrapeConfigSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.
      ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)
      scrapeConfigNamespaceSelector: {}
      ## Example which selects scrapeConfig in namespaces with label "prometheus" set to "somelabel"
      # scrapeConfigNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel
 
      ## How long to retain metrics
      ##
      retention: 10d
 
      ## Maximum size of metrics
      ##
      retentionSize: ""
 
      ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration
      ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb
      tsdb:
        outOfOrderTimeWindow: 0s
 
      ## Enable compression of the write-ahead log using Snappy.
      ##
      walCompression: true
 
      ## If true, the Operator won't process any Prometheus configuration changes
      ##
      paused: false
 
      ## Number of replicas of each shard to deploy for a Prometheus deployment.
      ## Number of replicas multiplied by shards is the total number of Pods created.
      ##
      replicas: 1
 
      ## EXPERIMENTAL: Number of shards to distribute targets onto.
      ## Number of replicas multiplied by shards is the total number of Pods created.
      ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
      ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
      ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
      ## Sharding is done on the content of the `__address__` target meta-label.
      ##
      shards: 1
 
      ## Log level for Prometheus be configured in
      ##
      logLevel: info
 
      ## Log format for Prometheus be configured in
      ##
      logFormat: logfmt
 
      ## Prefix used to register routes, overriding externalUrl route.
      ## Useful for proxies that rewrite URLs.
      ##
      routePrefix: /
 
      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the prometheus pods.
      ##
      podMetadata: {}
      # labels:
      #   app: prometheus
      #   k8s-app: prometheus
 
      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      podAntiAffinity: "soft"
 
      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname
 
      ## Assign custom affinity rules to the prometheus instance
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2
 
      ## The remote_read spec configuration for Prometheus.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#remotereadspec
      remoteRead: []
      # - url: http://remote1/read
      ## additionalRemoteRead is appended to remoteRead
      additionalRemoteRead: []
 
      ## The remote_write spec configuration for Prometheus.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#remotewritespec
      remoteWrite: []
      # - url: http://remote1/push
      ## additionalRemoteWrite is appended to remoteWrite
      additionalRemoteWrite: []
 
      ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
      remoteWriteDashboards: false
 
      ## Resource limits & requests
      ##
      resources: {}
      # requests:
      #   memory: 400Mi
 
      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/storage.md
      ##
      storageSpec: {}
      ## Using PersistentVolumeClaim
      ##
      #  volumeClaimTemplate:
      #    spec:
      #      storageClassName: gluster
      #      accessModes: ["ReadWriteOnce"]
      #      resources:
      #        requests:
      #          storage: 50Gi
      #    selector: {}
 
      ## Using tmpfs volume
      ##
      #  emptyDir:
      #    medium: Memory
 
      # Additional volumes on the output StatefulSet definition.
      volumes: []
 
      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []
 
      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs: []
      # - job_name: kube-etcd
      #   kubernetes_sd_configs:
      #     - role: node
      #   scheme: https
      #   tls_config:
      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
      #   relabel_configs:
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)
      #   - source_labels: [__address__]
      #     action: replace
      #     targetLabel: __address__
      #     regex: ([^:;]+):(\d+)
      #     replacement: ${1}:2379
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: keep
      #     regex: .*mst.*
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: replace
      #     targetLabel: node
      #     regex: (.*)
      #     replacement: ${1}
      #   metric_relabel_configs:
      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
      #     action: labeldrop
      #
      ## If scrape config contains a repetitive section, you may want to use a template.
      ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
      # additionalScrapeConfigs: |
      #  - job_name: "node-exporter"
      #    gce_sd_configs:
      #    {{range $zone := .Values.gcp_zones}}
      #    - project: "project1"
      #      zone: "{{$zone}}"
      #      port: 9100
      #    {{end}}
      #    relabel_configs:
      #    ...
 
 
      ## If additional scrape configurations are already deployed in a single secret file you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalScrapeConfigs
      additionalScrapeConfigsSecret: {}
        # enabled: false
        # name:
        # key:
 
      ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
      ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
      additionalPrometheusSecretsAnnotations: {}
 
      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alertmanager_config.
      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertManagerConfigs: []
      # - consul_sd_configs:
      #   - server: consul.dev.test:8500
      #     scheme: http
      #     datacenter: dev
      #     tag_separator: ','
      #     services:
      #       - metrics-prometheus-alertmanager
 
      ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertManagerConfigs
      additionalAlertManagerConfigsSecret: {}
        # name:
        # key:
        # optional: false
 
      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
      ## configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertRelabelConfigs: []
      # - separator: ;
      #   regex: prometheus_replica
      #   replacement: $1
      #   action: labeldrop
 
      ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertRelabelConfigs
      additionalAlertRelabelConfigsSecret: {}
        # name:
        # key:
 
      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
 
      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""
 
      ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
      ## This section is experimental, it may change significantly without deprecation notice in any release.
      ## This is experimental and may change significantly without backward compatibility in any release.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#thanosspec
      ##
      thanos: {}
        # secretProviderClass:
        #   provider: gcp
        #   parameters:
        #     secrets: |
        #       - resourceName: "projects/$PROJECT_ID/secrets/testsecret/versions/latest"
        #         fileName: "objstore.yaml"
        ## ObjectStorageConfig configures object storage in Thanos.
        # objectStorageConfig:
        #   # use existing secret, if configured, objectStorageConfig.secret will not be used
        #   existingSecret: {}
        #     # name: ""
        #     # key: ""
        #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,
        #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set
        #   # https://thanos.io/tip/thanos/storage.md/#s3
        #   secret: {}
        #     # type: S3
        #     # config:
        #     #   bucket: ""
        #     #   endpoint: ""
        #     #   region: ""
        #     #   access_key: ""
        #     #   secret_key: ""
 
      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
      ## if using proxy extraContainer update targetPort with proxy container port
      containers: []
      # containers:
      # - name: oauth-proxy
      #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.9.0
      #   args:
      #   - --upstream=http://127.0.0.1:9090
      #   - --http-address=0.0.0.0:8081
      #   - --metrics-address=0.0.0.0:8082
      #   - ...
      #   ports:
      #   - containerPort: 8081
      #     name: oauth-proxy
      #     protocol: TCP
      #   - containerPort: 8082
      #     name: oauth-metrics
      #     protocol: TCP
      #   resources: {}
 
      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []
 
      ## PortName to use for Prometheus.
      ##
      portName: "http-web"
 
      ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files
      ## on the file system of the Prometheus container e.g. bearer token files.
      arbitraryFSAccessThroughSMs: false
 
      ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor
      ## or PodMonitor to true, this overrides honor_labels to false.
      overrideHonorLabels: false
 
      ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.
      overrideHonorTimestamps: false
 
      ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,
      ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,
      ## and servicemonitors will be installed in the default service namespace.
      ## Defaults to false.
      ignoreNamespaceSelectors: false
 
      ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
      ## The label value will always be the namespace of the object that is being created.
      ## Disabled by default
      enforcedNamespaceLabel: ""
 
      ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.
      ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair
      ## Deprecated, use `excludedFromEnforcement` instead
      prometheusRulesExcludedFromEnforce: []
 
      ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects
      ## to be excluded from enforcing a namespace label of origin.
      ## Works only if enforcedNamespaceLabel set to true.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#objectreference
      excludedFromEnforcement: []
 
      ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,
      ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such
      ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions
      ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
      queryLogFile: false
 
      # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.
      # Set to 'false' to disable global sample_limit. or set to a number to override the default value.
      sampleLimit: false
 
      # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.
      # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets
      # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.
      enforcedKeepDroppedTargets: 0
 
      ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
      ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
      ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
      enforcedSampleLimit: false
 
      ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set
      ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall
      ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except
      ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.
      enforcedTargetLimit: false
 
 
      ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present
      ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
      ## 2.27.0 and newer.
      enforcedLabelLimit: false
 
      ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number
      ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
      ## 2.27.0 and newer.
      enforcedLabelNameLengthLimit: false
 
      ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this
      ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus
      ## versions 2.27.0 and newer.
      enforcedLabelValueLengthLimit: false
 
      ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental
      ## in Prometheus so it may change in any upcoming release.
      allowOverlappingBlocks: false
 
      ## Specifies the validation scheme for metric and label names.
      ## Supported values are: Legacy, UTF8
      nameValidationScheme: ""
 
      ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
      ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
      minReadySeconds: 0
 
      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
      # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.
      # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.
      hostNetwork: false
 
      # HostAlias holds the mapping between IP and hostnames that will be injected
      # as an entry in the pod's hosts file.
      hostAliases: []
      #  - ip: 10.10.0.100
      #    hostnames:
      #      - a1.app.local
      #      - b1.app.local
 
      ## TracingConfig configures tracing in Prometheus.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#prometheustracingconfig
      tracingConfig: {}
 
      ## Defines the service discovery role used to discover targets from ServiceMonitor objects and Alertmanager endpoints.
      ## If set, the value should be either "Endpoints" or "EndpointSlice". If unset, the operator assumes the "Endpoints" role.
      serviceDiscoveryRole: ""
 
      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}
 
      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating inside alertmanagerSpec.
      ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)
      additionalConfigString: ""
 
      ## Defines the maximum time that the `prometheus` container's startup probe
      ## will wait before being considered failed. The startup probe will return
      ## success after the WAL replay is complete. If set, the value should be
      ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15
      ## minutes).
      maximumStartupDurationSeconds: 0
 
    additionalRulesForClusterRole: []
    #  - apiGroups: [ "" ]
    #    resources:
    #      - nodes/proxy
    #    verbs: [ "get", "list", "watch" ]
 
    additionalServiceMonitors: []
    ## Name of the ServiceMonitor to create
    ##
    # - name: ""
 
      ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
      ## the chart
      ##
      # additionalLabels: {}
 
      ## Service label for use in assembling a job name of the form <label value>-<port>
      ## If no label is specified, the service name is used.
      ##
      # jobLabel: ""
 
      ## labels to transfer from the kubernetes service to the target
      ##
      # targetLabels: []
 
      ## labels to transfer from the kubernetes pods to the target
      ##
      # podTargetLabels: []
 
      ## Label selector for services to which this ServiceMonitor applies
      ##
      # selector: {}
        ## Example which selects all services to be monitored
        ## with label "monitoredby" with values any of "example-service-1" or "example-service-2"
        # matchExpressions:
        #   - key: "monitoredby"
        #     operator: In
        #     values:
        #       - example-service-1
        #       - example-service-2
 
        ## label selector for services
        ##
        # matchLabels: {}
 
      ## Namespaces from which services are selected
      ##
      # namespaceSelector:
        ## Match any namespace
        ##
        # any: false
 
        ## Explicit list of namespace names to select
        ##
        # matchNames: []
 
      ## Endpoints of the selected service to be monitored
      ##
      # endpoints: []
        ## Name of the endpoint's service port
        ## Mutually exclusive with targetPort
        # - port: ""
 
        ## Name or number of the endpoint's target port
        ## Mutually exclusive with port
        # - targetPort: ""
 
        ## File containing bearer token to be used when scraping targets
        ##
        #   bearerTokenFile: ""
 
        ## Interval at which metrics should be scraped
        ##
        #   interval: 30s
 
        ## HTTP path to scrape for metrics
        ##
        #   path: /metrics
 
        ## HTTP scheme to use for scraping
        ##
        #   scheme: http
 
        ## TLS configuration to use when scraping the endpoint
        ##
        #   tlsConfig:
 
            ## Path to the CA file
            ##
            # caFile: ""
 
            ## Path to client certificate file
            ##
            # certFile: ""
 
            ## Skip certificate verification
            ##
            # insecureSkipVerify: false
 
            ## Path to client key file
            ##
            # keyFile: ""
 
            ## Server name used to verify host name
            ##
            # serverName: ""
 
        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        # metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]
 
        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        # relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
 
      ## Fallback scrape protocol used by Prometheus for scraping metrics
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#monitoring.coreos.com/v1.ScrapeProtocol
      ##
      # fallbackScrapeProtocol: ""
 
    additionalPodMonitors: []
    ## Name of the PodMonitor to create
    ##
    # - name: ""
 
      ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
      ## the chart
      ##
      # additionalLabels: {}
 
      ## Pod label for use in assembling a job name of the form <label value>-<port>
      ## If no label is specified, the pod endpoint name is used.
      ##
      # jobLabel: ""
 
      ## Label selector for pods to which this PodMonitor applies
      ##
      # selector: {}
        ## Example which selects all Pods to be monitored
        ## with label "monitoredby" with values any of "example-pod-1" or "example-pod-2"
        # matchExpressions:
        #   - key: "monitoredby"
        #     operator: In
        #     values:
        #       - example-pod-1
        #       - example-pod-2
 
        ## label selector for pods
        ##
        # matchLabels: {}
 
      ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
      ##
      # podTargetLabels: {}
 
      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      # sampleLimit: 0
 
      ## Namespaces from which pods are selected
      ##
      # namespaceSelector:
        ## Match any namespace
        ##
        # any: false
 
        ## Explicit list of namespace names to select
        ##
        # matchNames: []
 
      ## Endpoints of the selected pods to be monitored
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#podmetricsendpoint
      ##
      # podMetricsEndpoints: []
 
      ## Fallback scrape protocol used by Prometheus for scraping metrics
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#monitoring.coreos.com/v1.ScrapeProtocol
      ##
      # fallbackScrapeProtocol: ""
 
  ## Configuration for thanosRuler
  ## ref: https://thanos.io/tip/components/rule.md/
  ##
  thanosRuler:
 
    ## Deploy thanosRuler
    ##
    enabled: false
 
    ## Annotations for ThanosRuler
    ##
    annotations: {}
 
    ## Service account for ThanosRuler to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
 
    ## Configure pod disruption budgets for ThanosRuler
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      # maxUnavailable: ""
      unhealthyPodEvictionPolicy: AlwaysAllow
 
    ingress:
      enabled: false
 
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
 
      annotations: {}
 
      labels: {}
 
      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts: []
        # - thanosruler.domain.com
 
      ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix
      ##
      paths: []
      # - /
 
      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific
 
      ## TLS configuration for ThanosRuler Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: thanosruler-general-tls
      #   hosts:
      #   - thanosruler.example.com
 
    # -- BETA: Configure the gateway routes for the chart here.
    # More routes can be added by adding a dictionary key like the 'main' route.
    # Be aware that this is an early beta of this feature,
    # kube-prometheus-stack does not guarantee this works and is subject to change.
    # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk
    # [[ref]](https://gateway-api.sigs.k8s.io/reference/spec/#gateway.networking.k8s.io%2fv1alpha2)
    route:
      main:
        # -- Enables or disables the route
        enabled: false
 
        # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2
        apiVersion: gateway.networking.k8s.io/v1
        # -- Set the route kind
        # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute
        kind: HTTPRoute
 
        annotations: {}
        labels: {}
 
        hostnames: []
        # - my-filter.example.com
        parentRefs: []
        # - name: acme-gw
 
        # -- create http route for redirect (https://gateway-api.sigs.k8s.io/guides/http-redirect-rewrite/#http-to-https-redirects)
        ## Take care that you only enable this on the http listener of the gateway to avoid an infinite redirect.
        ## matches, filters and additionalRules will be ignored if this is set to true. Be are
        httpsRedirect: false
 
        matches:
          - path:
              type: PathPrefix
              value: /
 
        ## Filters define the filters that are applied to requests that match this rule.
        filters: []
 
        ## Additional custom rules that can be added to the route
        additionalRules: []
 
    ## Configuration for ThanosRuler service
    ##
    service:
      enabled: true
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
 
      ## Port for ThanosRuler Service to listen on
      ##
      port: 10902
      ## To be used with a proxy extraContainer port
      ##
      targetPort: 10902
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30905
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
      ##
 
      ## Additional ports to open for ThanosRuler service
      additionalPorts: []
 
      externalIPs: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
 
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
 
      ## Service type
      ##
      type: ClusterIP
 
    ## Configuration for creating a ServiceMonitor for the ThanosRuler service
    ##
    serviceMonitor:
      ## If true, create a serviceMonitor for thanosRuler
      ##
      selfMonitor: true
 
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
 
      ## Additional labels
      ##
      additionalLabels: {}
 
      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0
 
      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0
 
      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0
 
      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0
 
      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0
 
      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
 
      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""
 
      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: {}
 
      bearerTokenFile:
 
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]
 
      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
 
      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics
 
    ## Settings affecting thanosRulerpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#thanosrulerspec
    ##
    thanosRulerSpec:
      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.
      ##
      podMetadata: {}
 
      ##
      serviceName:
 
      ## Image of ThanosRuler
      ##
      image:
        registry: quay.io
        repository: thanos/thanos
        tag: v0.38.0
        sha: ""
 
      ## Namespaces to be selected for PrometheusRules discovery.
      ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#namespaceselector for usage
      ##
      ruleNamespaceSelector: {}
 
      ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ##
      ruleSelectorNilUsesHelmValues: true
 
      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector: {}
      ## Example which select all PrometheusRules resources
      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
      # ruleSelector:
      #   matchExpressions:
      #     - key: prometheus
      #       operator: In
      #       values:
      #         - example-rules
      #         - example-rules-2
      #
      ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
      # ruleSelector:
      #   matchLabels:
      #     role: example-rules
 
      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: logfmt
 
      ## Log level for ThanosRuler to be configured with.
      ##
      logLevel: info
 
      ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1
 
      ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 24h
 
      ## Interval between consecutive evaluations.
      ##
      evaluationInterval: ""
 
      ## Storage is the definition of how storage will be used by the ThanosRuler instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/storage.md
      ##
      storage: {}
      # volumeClaimTemplate:
      #   spec:
      #     storageClassName: gluster
      #     accessModes: ["ReadWriteOnce"]
      #     resources:
      #       requests:
      #         storage: 50Gi
      #   selector: {}
 
      ## AlertmanagerConfig define configuration for connecting to alertmanager.
      ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.
      alertmanagersConfig:
        # use existing secret, if configured, alertmanagersConfig.secret will not be used
        existingSecret: {}
          # name: ""
          # key: ""
        # will render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set
        # https://thanos.io/tip/components/rule.md/#alertmanager
        secret: {}
          # alertmanagers:
          # - api_version: v2
          #   http_config:
          #     basic_auth:
          #       username: some_user
          #       password: some_pass
          #   static_configs:
          #     - alertmanager.thanos.io
          #   scheme: http
          #   timeout: 10s
 
      ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.
      ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.
      # alertmanagersUrl:
 
      ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false
      ##
      externalPrefix:
 
      ## If true, http://{{ template "kube-prometheus-stack.thanosRuler.name" . }}.{{ template "kube-prometheus-stack.namespace" . }}:{{ .Values.thanosRuler.service.port }}
      ## will be used as value for externalPrefix
      externalPrefixNilUsesHelmValues: true
 
      ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
      ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
      ##
      routePrefix: /
 
      ## ObjectStorageConfig configures object storage in Thanos
      objectStorageConfig:
        # use existing secret, if configured, objectStorageConfig.secret will not be used
        existingSecret: {}
          # name: ""
          # key: ""
        # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set
        # https://thanos.io/tip/thanos/storage.md/#s3
        secret: {}
          # type: S3
          # config:
          #   bucket: ""
          #   endpoint: ""
          #   region: ""
          #   access_key: ""
          #   secret_key: ""
 
      ## Labels by name to drop before sending to alertmanager
      ## Maps to the --alert.label-drop flag of thanos ruler.
      alertDropLabels: []
 
      ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.
      ## Maps to the --query flag of thanos ruler.
      queryEndpoints: []
 
      ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
      ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
      queryConfig:
        # use existing secret, if configured, queryConfig.secret will not be used
        existingSecret: {}
          # name: ""
          # key: ""
        # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set
        # https://thanos.io/tip/components/rule.md/#query-api
        secret: {}
          # - http_config:
          #     basic_auth:
          #       username: some_user
          #       password: some_pass
          #   static_configs:
          #     - URL
          #   scheme: http
          #   timeout: 10s
 
      ## Labels configure the external label pairs to ThanosRuler. A default replica
      ## label `thanos_ruler_replica` will be always added as a label with the value
      ## of the pod's name and it will be dropped in the alerts.
      labels: {}
 
      ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
      ##
      paused: false
 
      ## Allows setting additional arguments for the ThanosRuler container
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#thanosruler
      ##
      additionalArgs: []
        # - name: remote-write.config
        #   value: |-
        #     "remote_write":
        #     - "name": "receiver-0"
        #       "remote_timeout": "30s"
        #       "url": "http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive"
 
      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
      ##
      nodeSelector: {}
 
      ## Define resources requests and limits for single Pods.
      ## ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
      ##
      resources: {}
      # requests:
      #   memory: 400Mi
 
      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      ##
      podAntiAffinity: "soft"
 
      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname
 
      ## Assign custom affinity rules to the thanosRuler instance
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2
 
      ## If specified, the pod's tolerations.
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
      ##
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"
 
      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: thanos-ruler
 
      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
 
      ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.
      ## Note this is only for the ThanosRuler UI, not the gossip communication.
      ##
      listenLocal: false
 
      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.
      ##
      containers: []
 
      # Additional volumes on the output StatefulSet definition.
      volumes: []
 
      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []
 
      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []
 
      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""
 
      ## PortName to use for ThanosRuler.
      ##
      portName: "web"
 
      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#thanosrulerwebspec
      web: {}
 
      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}
 
      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating
      additionalConfigString: ""
 
    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.
 
  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  ##
  cleanPrometheusOperatorObjectNames: false
 
  ## Extra manifests to deploy.  Can be of type dict or list.
  ## If dict, keys are ignored and only values are used.
  ## Items contained within extraObjects can be defined as dict or string and are passed through tpl.
  extraManifests: null
    # - apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #   labels:
    #     name: prometheus-extra
    #   data:
    #     extra-data: "value"
    #
    # can also be defined as a string, useful for templating field names
    # - |
    #   apiVersion: v1
    #   kind: Secret
    #   type: Opaque
    #   metadata:
    #     name: super-secret
    #     labels:
    #       {{- range $key, $value := .Values.commonLabels }}
    #       {{ $key }}: {{ $value }}
    #       {{- end }}
    #   data:
    #     plaintext: Zm9vYmFy
    #     templated: '{{ print "foobar" | upper | b64enc }}'
  